{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from aggregated_embeddings_with_labels.pkl...\n",
      "Loaded 2137 PeriodIDs.\n",
      "Loading embeddings from aggregated_embeddings_with_labels_test.pkl...\n",
      "Loaded 516 PeriodIDs.\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "def load_embeddings(file_path):\n",
    "    \"\"\"\n",
    "    Load embeddings and labels from the pickle file and extract MatchID from the PeriodID.\n",
    "    \"\"\"\n",
    "    print(f\"Loading embeddings from {file_path}...\")\n",
    "    merged_df = pd.read_pickle(file_path)\n",
    "    if \"ID\" in merged_df.columns:\n",
    "        merged_df[\"MatchID\"] = merged_df[\"ID\"].apply(lambda x: x.split(\"_\")[0])\n",
    "    print(f\"Loaded {len(merged_df)} PeriodIDs.\")\n",
    "    return merged_df\n",
    "\n",
    "train_embeddings_file = \"aggregated_embeddings_with_labels.pkl\"\n",
    "test_embeddings_file = \"aggregated_embeddings_with_labels_test.pkl\"\n",
    "\n",
    "train_df = load_embeddings(train_embeddings_file)\n",
    "test_df = load_embeddings(test_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_game_based_data_loaders(df, batch_size, sequence_length=5, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create DataLoaders grouped by game. Each DataLoader contains sequences of embeddings per game.\n",
    "    \"\"\"\n",
    "    game_loaders = {}\n",
    "    grouped = df.groupby(\"MatchID\")\n",
    "\n",
    "    for match_id, group in grouped:\n",
    "        all_embeddings = np.vstack(group[\"aggregated_embedding\"].values).astype(np.float32)\n",
    "        all_labels = group[\"EventType\"].values.astype(int)\n",
    "\n",
    "        sequences = []\n",
    "        seq_labels = []\n",
    "\n",
    "        # Create sequences\n",
    "        for i in range(len(all_embeddings) - sequence_length + 1):\n",
    "            seq_emb = all_embeddings[i:i+sequence_length]\n",
    "            seq_label = all_labels[i+sequence_length-1]\n",
    "            sequences.append(seq_emb)\n",
    "            seq_labels.append(seq_label)\n",
    "\n",
    "        if len(sequences) == 0:\n",
    "            # If a game is too short, skip it\n",
    "            continue\n",
    "\n",
    "        sequences = np.array(sequences)\n",
    "        seq_labels = np.array(seq_labels)\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(sequences, dtype=torch.float32),\n",
    "            torch.tensor(seq_labels, dtype=torch.float32)\n",
    "        )\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        game_loaders[match_id] = loader\n",
    "\n",
    "    return game_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually split into train/val/test based on MatchID as requested\n",
    "all_games = train_df[\"MatchID\"].unique()\n",
    "# Suppose we have at least 16 games as mentioned.\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(all_games)\n",
    "\n",
    "# Take 3 games for final testing (unseen completely)\n",
    "final_test_games = all_games[:3]\n",
    "train_val_games = all_games[3:]\n",
    "\n",
    "# Out of these 13 games, let's do a simple train/val split (e.g. 20% for validation)\n",
    "val_ratio = 0.2\n",
    "val_count = int(len(train_val_games) * val_ratio)\n",
    "val_games = train_val_games[:val_count]\n",
    "train_games = train_val_games[val_count:]\n",
    "\n",
    "train_df_cv = train_df[train_df[\"MatchID\"].isin(train_games)]\n",
    "val_df_cv = train_df[train_df[\"MatchID\"].isin(val_games)]\n",
    "\n",
    "# Final test set (held-out before Kaggle submission)\n",
    "final_test_df = train_df[train_df[\"MatchID\"].isin(final_test_games)]\n",
    "\n",
    "# Create data loaders\n",
    "train_game_loaders = create_game_based_data_loaders(train_df_cv, batch_size=32, sequence_length=3)\n",
    "val_game_loaders = create_game_based_data_loaders(val_df_cv, batch_size=32, sequence_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=84, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class ChronologicalLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=128, num_layers=2, bidirectional=True, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        self.intermediate_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc1 = nn.Linear(self.intermediate_dim, self.intermediate_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.classifier = nn.Linear(self.intermediate_dim, 1)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        _, (hidden, _) = self.lstm(embeddings)\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden_state = torch.cat((hidden[-2], hidden[-1]), dim=-1)\n",
    "        else:\n",
    "            hidden_state = hidden[-1]\n",
    "        x = self.fc1(hidden_state)\n",
    "        x = self.activation(x)\n",
    "        logits = self.classifier(x)\n",
    "        return torch.sigmoid(logits).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Model\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_heads=4, hidden_dim=128, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.projection = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = PositionalEncoding(d_model=hidden_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        x = self.projection(embeddings)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        pooled_output = x.mean(dim=1)\n",
    "        x = self.fc1(pooled_output)\n",
    "        x = self.activation(x)\n",
    "        logits = self.classifier(x)\n",
    "        return torch.sigmoid(logits).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_game_loaders, device):\n",
    "    model.eval()\n",
    "    val_loss, val_accuracy = 0, 0\n",
    "    total_val_batches = 0\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for match_id, val_loader in val_game_loaders.items():\n",
    "            for embeddings, labels in val_loader:\n",
    "                embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "                outputs = model(embeddings)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = (outputs.cpu().numpy() > 0.5).astype(int)\n",
    "                val_accuracy += accuracy_score(labels.cpu().numpy(), preds)\n",
    "                total_val_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss / total_val_batches if total_val_batches > 0 else 0\n",
    "    avg_val_acc = val_accuracy / total_val_batches if total_val_batches > 0 else 0\n",
    "    return avg_val_loss, avg_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_game_based_model(model, train_loaders, val_loaders, epochs, lr, device, patience=10, weight_decay=1e-5):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Early stopping initialization\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        total_train_batches = 0\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}...\")\n",
    "        for match_id, game_loader in train_loaders.items():\n",
    "            for embeddings, labels in game_loader:\n",
    "                embeddings, labels = embeddings.to(device), labels.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(embeddings)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                preds = (outputs.detach().cpu().numpy() > 0.5).astype(int)\n",
    "                train_accuracy += accuracy_score(labels.cpu().numpy(), preds)\n",
    "                total_train_batches += 1\n",
    "\n",
    "        avg_train_loss = train_loss / total_train_batches\n",
    "        avg_train_acc = train_accuracy / total_train_batches\n",
    "\n",
    "        # Validation\n",
    "        avg_val_loss, avg_val_acc = evaluate_model(model, val_loaders, device)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss {avg_train_loss:.4f}, Train Acc {avg_train_acc:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Acc {avg_val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_acc > best_val_acc:\n",
    "            best_val_acc = avg_val_acc\n",
    "            patience_counter = 0\n",
    "            # Save best model weights\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    if 'best_model_state' in locals():\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_comparison(models, val_game_loaders, device):\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for match_id, val_loader in val_game_loaders.items():\n",
    "                for embeddings, labels in val_loader:\n",
    "                    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "                    outputs = model(embeddings)\n",
    "                    preds = (outputs > 0.5).int().cpu().numpy()\n",
    "                    all_preds.extend(preds)\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna   # Hyperparameter tuning library\n",
    "\n",
    "# ===========================\n",
    "# Hyperparameter Tuning Setup\n",
    "# ===========================\n",
    "def objective(trial, train_game_loaders, val_game_loaders, device):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "    # Ensure hidden_dim is even\n",
    "    if hidden_dim % 2 != 0:\n",
    "        hidden_dim += 1\n",
    "    model_type = trial.suggest_categorical(\"model_type\", [\"LSTM\", \"Transformer\"])\n",
    "\n",
    "    if model_type == \"LSTM\":\n",
    "        model = ChronologicalLSTM(input_dim=768, hidden_dim=hidden_dim, num_layers=1, bidirectional=True, dropout=0.3)\n",
    "    else:\n",
    "        num_heads = trial.suggest_categorical(\"num_heads\", [2,4])\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "        # Ensure hidden_dim is divisible by num_heads if needed\n",
    "        if hidden_dim % num_heads != 0:\n",
    "            # Adjust hidden_dim to be divisible by num_heads\n",
    "            hidden_dim += (num_heads - (hidden_dim % num_heads))\n",
    "        model = TransformerEncoder(input_dim=768, hidden_dim=hidden_dim, num_heads=num_heads, num_layers=num_layers, dropout=0.3)\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    epochs = 5\n",
    "    best_val_acc = 0.0\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for match_id, game_loader in train_game_loaders.items():\n",
    "            for embeddings, labels in game_loader:\n",
    "                embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(embeddings)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_accuracy = 0\n",
    "        total_val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for match_id, val_loader in val_game_loaders.items():\n",
    "                for embeddings, labels in val_loader:\n",
    "                    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "                    outputs = model(embeddings)\n",
    "                    preds = (outputs > 0.5).int().cpu().numpy()\n",
    "                    val_accuracy += accuracy_score(labels.cpu().numpy(), preds)\n",
    "                    total_val_batches += 1\n",
    "\n",
    "        avg_val_acc = val_accuracy / total_val_batches if total_val_batches > 0 else 0\n",
    "        if avg_val_acc > best_val_acc:\n",
    "            best_val_acc = avg_val_acc\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "def run_hyperparameter_tuning(train_game_loaders, val_game_loaders, device, n_trials=20):\n",
    "    # Call this function after you have train_game_loaders and val_game_loaders defined\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, train_game_loaders, val_game_loaders, device), n_trials=n_trials)\n",
    "    return study.best_params\n",
    "\n",
    "# ===========================\n",
    "# Training on Final Data\n",
    "# ===========================\n",
    "def train_on_final_data(model, final_test_loaders, epochs=5, lr=1e-4, device=\"cpu\", weight_decay=1e-5):\n",
    "    # This function will train the model on the final_test_loaders after evaluating on them,\n",
    "    # incorporating all data into the model before submission.\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss, total_train_acc = 0, 0\n",
    "        total_train_batches = 0\n",
    "\n",
    "        for match_id, loader in final_test_loaders.items():\n",
    "            for embeddings, labels in loader:\n",
    "                embeddings, labels = embeddings.to(device), labels.to(device).float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(embeddings)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                preds = (outputs.detach().cpu().numpy() > 0.5).astype(int)\n",
    "                total_train_acc += accuracy_score(labels.cpu().numpy(), preds)\n",
    "                total_train_batches += 1\n",
    "\n",
    "        avg_train_loss = total_train_loss / total_train_batches if total_train_batches > 0 else 0\n",
    "        avg_train_acc = total_train_acc / total_train_batches if total_train_batches > 0 else 0\n",
    "        print(f\"Final Data Training Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Chronological LSTM...\n",
      "Epoch 1/50...\n",
      "Epoch 1: Train Loss 0.6902, Train Acc 0.5463\n",
      "Validation Loss: 0.6879, Validation Acc 0.5508\n",
      "Epoch 2/50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 0.6871, Train Acc 0.5525\n",
      "Validation Loss: 0.6875, Validation Acc 0.5508\n",
      "Epoch 3/50...\n",
      "Epoch 3: Train Loss 0.6840, Train Acc 0.5524\n",
      "Validation Loss: 0.6861, Validation Acc 0.5508\n",
      "Epoch 4/50...\n",
      "Epoch 4: Train Loss 0.6751, Train Acc 0.5769\n",
      "Validation Loss: 0.6813, Validation Acc 0.6289\n",
      "Epoch 5/50...\n",
      "Epoch 5: Train Loss 0.6616, Train Acc 0.6011\n",
      "Validation Loss: 0.6751, Validation Acc 0.6055\n",
      "Epoch 6/50...\n",
      "Epoch 6: Train Loss 0.6436, Train Acc 0.6310\n",
      "Validation Loss: 0.6813, Validation Acc 0.5195\n",
      "Epoch 7/50...\n",
      "Epoch 7: Train Loss 0.6350, Train Acc 0.6491\n",
      "Validation Loss: 0.6440, Validation Acc 0.6406\n",
      "Epoch 8/50...\n",
      "Epoch 8: Train Loss 0.6320, Train Acc 0.6360\n",
      "Validation Loss: 0.6437, Validation Acc 0.6133\n",
      "Epoch 9/50...\n",
      "Epoch 9: Train Loss 0.6261, Train Acc 0.6318\n",
      "Validation Loss: 0.6410, Validation Acc 0.6172\n",
      "Epoch 10/50...\n",
      "Epoch 10: Train Loss 0.6324, Train Acc 0.6429\n",
      "Validation Loss: 0.6754, Validation Acc 0.5391\n",
      "Epoch 11/50...\n",
      "Epoch 11: Train Loss 0.6111, Train Acc 0.6582\n",
      "Validation Loss: 0.6501, Validation Acc 0.6055\n",
      "Epoch 12/50...\n",
      "Epoch 12: Train Loss 0.5968, Train Acc 0.6699\n",
      "Validation Loss: 0.6636, Validation Acc 0.5898\n",
      "Early stopping triggered.\n",
      "Training Transformer Encoder...\n",
      "Epoch 1/50...\n",
      "Epoch 1: Train Loss 0.6966, Train Acc 0.5150\n",
      "Validation Loss: 0.6890, Validation Acc 0.5508\n",
      "Epoch 2/50...\n",
      "Epoch 2: Train Loss 0.6925, Train Acc 0.5282\n",
      "Validation Loss: 0.6936, Validation Acc 0.4492\n",
      "Epoch 3/50...\n",
      "Epoch 3: Train Loss 0.6920, Train Acc 0.5498\n",
      "Validation Loss: 0.6912, Validation Acc 0.5508\n",
      "Epoch 4/50...\n",
      "Epoch 4: Train Loss 0.6908, Train Acc 0.5442\n",
      "Validation Loss: 0.6886, Validation Acc 0.5508\n",
      "Epoch 5/50...\n",
      "Epoch 5: Train Loss 0.6885, Train Acc 0.5484\n",
      "Validation Loss: 0.6885, Validation Acc 0.5508\n",
      "Epoch 6/50...\n",
      "Epoch 6: Train Loss 0.6869, Train Acc 0.5511\n",
      "Validation Loss: 0.6903, Validation Acc 0.6289\n",
      "Epoch 7/50...\n",
      "Epoch 7: Train Loss 0.6840, Train Acc 0.5636\n",
      "Validation Loss: 0.6843, Validation Acc 0.5508\n",
      "Epoch 8/50...\n",
      "Epoch 8: Train Loss 0.6804, Train Acc 0.5761\n",
      "Validation Loss: 0.6863, Validation Acc 0.5430\n",
      "Epoch 9/50...\n",
      "Epoch 9: Train Loss 0.6641, Train Acc 0.5983\n",
      "Validation Loss: 0.8049, Validation Acc 0.4492\n",
      "Epoch 10/50...\n",
      "Epoch 10: Train Loss 0.6651, Train Acc 0.6018\n",
      "Validation Loss: 0.6904, Validation Acc 0.5156\n",
      "Epoch 11/50...\n",
      "Epoch 11: Train Loss 0.6575, Train Acc 0.6102\n",
      "Validation Loss: 0.7349, Validation Acc 0.4922\n",
      "Early stopping triggered.\n",
      "                 Model  Accuracy  Precision    Recall  F1-Score\n",
      "0   Chronological LSTM  0.589844   0.833333  0.319149  0.461538\n",
      "1  Transformer Encoder  0.492188   0.823529  0.099291  0.177215\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Main Training & Evaluation\n",
    "# ===========================\n",
    "print(\"Training Chronological LSTM...\")\n",
    "lstm_model = ChronologicalLSTM(input_dim=768, hidden_dim=128, num_layers=1, bidirectional=True, dropout=0.3)\n",
    "trained_lstm_model = train_game_based_model(lstm_model, train_game_loaders, val_game_loaders, epochs=50, lr=1e-4, device=device, patience=5)\n",
    "\n",
    "print(\"Training Transformer Encoder...\")\n",
    "transformer_model = TransformerEncoder(input_dim=768, hidden_dim=128, num_heads=4, num_layers=2, dropout=0.3)\n",
    "trained_transformer_model = train_game_based_model(transformer_model, train_game_loaders, val_game_loaders, epochs=50, lr=1e-4, device=device, patience=5)\n",
    "\n",
    "comparison_df = evaluate_model_comparison(\n",
    "    {\"Chronological LSTM\": trained_lstm_model, \"Transformer Encoder\": trained_transformer_model},\n",
    "    val_game_loaders, device\n",
    ")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Unseen Test Set Results:\n",
      "                 Model  Accuracy  Precision    Recall  F1-Score\n",
      "0   Chronological LSTM  0.550691   0.677419  0.276316  0.392523\n",
      "1  Transformer Encoder  0.490783   1.000000  0.030702  0.059574\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on final unseen test set\n",
    "final_test_loaders = create_game_based_data_loaders(final_test_df, batch_size=32, sequence_length=3, shuffle=False)\n",
    "final_comparison_df = evaluate_model_comparison(\n",
    "    {\"Chronological LSTM\": trained_lstm_model, \"Transformer Encoder\": trained_transformer_model},\n",
    "    final_test_loaders, device\n",
    ")\n",
    "print(\"Final Unseen Test Set Results:\")\n",
    "print(final_comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 00:26:49,429] A new study created in memory with name: no-name-b82ab3af-9354-40c1-b930-df0a337dadd3\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:26:49,922] Trial 0 finished with value: 0.55078125 and parameters: {'lr': 0.00032934274357231303, 'hidden_dim': 69, 'model_type': 'LSTM'}. Best is trial 0 with value: 0.55078125.\n",
      "[I 2024-12-07 00:26:51,866] Trial 1 finished with value: 0.55078125 and parameters: {'lr': 5.1943597765505735e-05, 'hidden_dim': 146, 'model_type': 'Transformer', 'num_heads': 2, 'num_layers': 3}. Best is trial 0 with value: 0.55078125.\n",
      "[I 2024-12-07 00:26:52,931] Trial 2 finished with value: 0.55078125 and parameters: {'lr': 0.0004896847728778276, 'hidden_dim': 216, 'model_type': 'Transformer', 'num_heads': 2, 'num_layers': 2}. Best is trial 0 with value: 0.55078125.\n",
      "[I 2024-12-07 00:26:54,723] Trial 3 finished with value: 0.55078125 and parameters: {'lr': 0.0008215026119212586, 'hidden_dim': 227, 'model_type': 'Transformer', 'num_heads': 2, 'num_layers': 3}. Best is trial 0 with value: 0.55078125.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:26:55,195] Trial 4 finished with value: 0.5703125 and parameters: {'lr': 9.430687243507303e-05, 'hidden_dim': 128, 'model_type': 'LSTM'}. Best is trial 4 with value: 0.5703125.\n",
      "[I 2024-12-07 00:26:56,492] Trial 5 finished with value: 0.5859375 and parameters: {'lr': 5.929197811918972e-05, 'hidden_dim': 214, 'model_type': 'Transformer', 'num_heads': 4, 'num_layers': 2}. Best is trial 5 with value: 0.5859375.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:26:56,932] Trial 6 finished with value: 0.609375 and parameters: {'lr': 0.00054466963062734, 'hidden_dim': 207, 'model_type': 'LSTM'}. Best is trial 6 with value: 0.609375.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:26:57,329] Trial 7 finished with value: 0.578125 and parameters: {'lr': 0.0001720313949297659, 'hidden_dim': 159, 'model_type': 'LSTM'}. Best is trial 6 with value: 0.609375.\n",
      "[I 2024-12-07 00:26:59,005] Trial 8 finished with value: 0.55078125 and parameters: {'lr': 1.1604152202822839e-05, 'hidden_dim': 241, 'model_type': 'Transformer', 'num_heads': 2, 'num_layers': 3}. Best is trial 6 with value: 0.609375.\n",
      "[I 2024-12-07 00:27:00,469] Trial 9 finished with value: 0.55078125 and parameters: {'lr': 4.238776283174931e-05, 'hidden_dim': 75, 'model_type': 'Transformer', 'num_heads': 4, 'num_layers': 3}. Best is trial 6 with value: 0.609375.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:27:01,004] Trial 10 finished with value: 0.55078125 and parameters: {'lr': 0.00023423122799329487, 'hidden_dim': 190, 'model_type': 'LSTM'}. Best is trial 6 with value: 0.609375.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:27:01,580] Trial 11 finished with value: 0.55078125 and parameters: {'lr': 2.3646485523295153e-05, 'hidden_dim': 194, 'model_type': 'LSTM'}. Best is trial 6 with value: 0.609375.\n",
      "[I 2024-12-07 00:27:02,648] Trial 12 finished with value: 0.55078125 and parameters: {'lr': 0.00011697600844996295, 'hidden_dim': 202, 'model_type': 'Transformer', 'num_heads': 4, 'num_layers': 1}. Best is trial 6 with value: 0.609375.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:27:03,203] Trial 13 finished with value: 0.56640625 and parameters: {'lr': 5.2708881829565374e-05, 'hidden_dim': 250, 'model_type': 'LSTM'}. Best is trial 6 with value: 0.609375.\n",
      "[I 2024-12-07 00:27:04,212] Trial 14 finished with value: 0.55078125 and parameters: {'lr': 0.0008440218948081762, 'hidden_dim': 169, 'model_type': 'Transformer', 'num_heads': 4, 'num_layers': 1}. Best is trial 6 with value: 0.609375.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:27:04,739] Trial 15 finished with value: 0.62890625 and parameters: {'lr': 9.483162463996305e-05, 'hidden_dim': 126, 'model_type': 'LSTM'}. Best is trial 15 with value: 0.62890625.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:27:05,659] Trial 16 finished with value: 0.55078125 and parameters: {'lr': 0.0004384968826246849, 'hidden_dim': 114, 'model_type': 'LSTM'}. Best is trial 15 with value: 0.62890625.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:27:06,207] Trial 17 finished with value: 0.58984375 and parameters: {'lr': 0.00012601947358387378, 'hidden_dim': 101, 'model_type': 'LSTM'}. Best is trial 15 with value: 0.62890625.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:27:06,736] Trial 18 finished with value: 0.55078125 and parameters: {'lr': 2.546744516916229e-05, 'hidden_dim': 139, 'model_type': 'LSTM'}. Best is trial 15 with value: 0.62890625.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2024-12-07 00:27:07,255] Trial 19 finished with value: 0.6015625 and parameters: {'lr': 0.00022683927312481007, 'hidden_dim': 177, 'model_type': 'LSTM'}. Best is trial 15 with value: 0.62890625.\n",
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters found by Optuna: {'lr': 9.483162463996305e-05, 'hidden_dim': 126, 'model_type': 'LSTM'}\n",
      "Epoch 1/50...\n",
      "Epoch 1: Train Loss 0.6904, Train Acc 0.5504\n",
      "Validation Loss: 0.6885, Validation Acc 0.5508\n",
      "Epoch 2/50...\n",
      "Epoch 2: Train Loss 0.6874, Train Acc 0.5566\n",
      "Validation Loss: 0.6872, Validation Acc 0.5508\n",
      "Epoch 3/50...\n",
      "Epoch 3: Train Loss 0.6857, Train Acc 0.5525\n",
      "Validation Loss: 0.6870, Validation Acc 0.5508\n",
      "Epoch 4/50...\n",
      "Epoch 4: Train Loss 0.6808, Train Acc 0.5588\n",
      "Validation Loss: 0.6842, Validation Acc 0.5508\n",
      "Epoch 5/50...\n",
      "Epoch 5: Train Loss 0.6737, Train Acc 0.5718\n",
      "Validation Loss: 0.6799, Validation Acc 0.5586\n",
      "Epoch 6/50...\n",
      "Epoch 6: Train Loss 0.6610, Train Acc 0.5879\n",
      "Validation Loss: 0.6791, Validation Acc 0.5781\n",
      "Epoch 7/50...\n",
      "Epoch 7: Train Loss 0.6471, Train Acc 0.6261\n",
      "Validation Loss: 0.6557, Validation Acc 0.6133\n",
      "Epoch 8/50...\n",
      "Epoch 8: Train Loss 0.6426, Train Acc 0.6274\n",
      "Validation Loss: 0.6525, Validation Acc 0.6094\n",
      "Epoch 9/50...\n",
      "Epoch 9: Train Loss 0.6426, Train Acc 0.6387\n",
      "Validation Loss: 0.6751, Validation Acc 0.5586\n",
      "Epoch 10/50...\n",
      "Epoch 10: Train Loss 0.6302, Train Acc 0.6330\n",
      "Validation Loss: 0.7011, Validation Acc 0.5000\n",
      "Epoch 11/50...\n",
      "Epoch 11: Train Loss 0.6216, Train Acc 0.6353\n",
      "Validation Loss: 0.7005, Validation Acc 0.5000\n",
      "Epoch 12/50...\n",
      "Epoch 12: Train Loss 0.6343, Train Acc 0.5976\n",
      "Validation Loss: 0.6859, Validation Acc 0.5078\n",
      "Early stopping triggered.\n",
      "Final Unseen Test Set Results with Finetuned Best Model:\n",
      "        Model  Accuracy  Precision   Recall  F1-Score\n",
      "0  Best Model  0.497696        1.0  0.04386  0.084034\n",
      "Final Data Training Epoch 1/5 - Train Loss: 0.6993, Train Acc: 0.4606\n",
      "Final Data Training Epoch 2/5 - Train Loss: 0.6752, Train Acc: 0.5997\n",
      "Final Data Training Epoch 3/5 - Train Loss: 0.6816, Train Acc: 0.5131\n",
      "Final Data Training Epoch 4/5 - Train Loss: 0.6612, Train Acc: 0.5833\n",
      "Final Data Training Epoch 5/5 - Train Loss: 0.6638, Train Acc: 0.5551\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Hyperparameter Tuning (Call before final test)\n",
    "# =============================================\n",
    "best_params = run_hyperparameter_tuning(train_game_loaders, val_game_loaders, device, n_trials=20)\n",
    "print(\"Best Hyperparameters found by Optuna:\", best_params)\n",
    "\n",
    "# After you get best_params, automatically re-initialize the best model based on model_type\n",
    "model_type = best_params[\"model_type\"]\n",
    "if model_type == \"LSTM\":\n",
    "    best_model = ChronologicalLSTM(\n",
    "        input_dim=768, \n",
    "        hidden_dim=best_params[\"hidden_dim\"], \n",
    "        num_layers=1, \n",
    "        bidirectional=True, \n",
    "        dropout=0.3\n",
    "    )\n",
    "else:\n",
    "    best_model = TransformerEncoder(\n",
    "        input_dim=768,\n",
    "        hidden_dim=best_params[\"hidden_dim\"],\n",
    "        num_heads=best_params[\"num_heads\"],\n",
    "        num_layers=best_params[\"num_layers\"],\n",
    "        dropout=0.3\n",
    "    )\n",
    "\n",
    "# Re-train the best model on the training sets with best parameters\n",
    "best_model = train_game_based_model(best_model, train_game_loaders, val_game_loaders, epochs=50, lr=best_params[\"lr\"], device=device, patience=5)\n",
    "\n",
    "# Evaluate the fine-tuned best model on the final_test_loaders (3 unseen games)\n",
    "final_test_results = evaluate_model_comparison({\"Best Model\": best_model}, final_test_loaders, device)\n",
    "print(\"Final Unseen Test Set Results with Finetuned Best Model:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the best model on the final_test_loaders to use your full dataset\n",
    "best_model = train_on_final_data(best_model, final_test_loaders, epochs=5, lr=1e-4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as final_submission.csv.\n"
     ]
    }
   ],
   "source": [
    "def predict_for_kaggle_submission(model, test_df, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        grouped = test_df.groupby(\"MatchID\")\n",
    "        for match_id, group in grouped:\n",
    "            embeddings = np.vstack(group[\"aggregated_embedding\"].values).astype(np.float32)\n",
    "            embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "            # One-step predictions\n",
    "            outputs = model(embeddings_tensor.unsqueeze(1)).squeeze(-1)\n",
    "            preds = (outputs.cpu().numpy() > 0.5).astype(int)\n",
    "            group[\"EventType\"] = preds\n",
    "            predictions.append(group[[\"ID\", \"EventType\"]])\n",
    "    return pd.concat(predictions)\n",
    "\n",
    "submission_df = predict_for_kaggle_submission(trained_lstm_model, test_df, device)\n",
    "submission_df.to_csv(\"final_submission.csv\", index=False)\n",
    "print(\"Submission file saved as final_submission.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
