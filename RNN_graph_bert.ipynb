{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./train_tweets/MexicoCroatia37.csv for game MexicoCroatia37...\n",
      "Loading ./train_tweets/AustraliaSpain34.csv for game AustraliaSpain34...\n",
      "Loading ./train_tweets/GermanyBrazil74.csv for game GermanyBrazil74...\n",
      "Loading ./train_tweets/GermanyUSA57.csv for game GermanyUSA57...\n",
      "Loading ./train_tweets/FranceGermany70.csv for game FranceGermany70...\n",
      "Loading ./train_tweets/PortugalGhana58.csv for game PortugalGhana58...\n",
      "Loading ./train_tweets/CameroonBrazil36.csv for game CameroonBrazil36...\n",
      "Loading ./train_tweets/NetherlandsChile35.csv for game NetherlandsChile35...\n",
      "Loading ./train_tweets/BelgiumSouthKorea59.csv for game BelgiumSouthKorea59...\n",
      "Loading ./train_tweets/USASlovenia2010.csv for game USASlovenia2010...\n",
      "Loading ./train_tweets/AustraliaNetherlands29.csv for game AustraliaNetherlands29...\n",
      "Loading ./train_tweets/ArgentinaBelgium72.csv for game ArgentinaBelgium72...\n",
      "Loading ./train_tweets/HondurasSwitzerland54.csv for game HondurasSwitzerland54...\n",
      "Loading ./train_tweets/FranceNigeria66.csv for game FranceNigeria66...\n",
      "Loading ./train_tweets/ArgentinaGermanyFinal77.csv for game ArgentinaGermanyFinal77...\n",
      "Loading ./train_tweets/GermanyAlgeria67.csv for game GermanyAlgeria67...\n",
      "Loading ./eval_tweets/NetherlandsMexico64.csv for game NetherlandsMexico64...\n",
      "Loading ./eval_tweets/GermanySerbia2010.csv for game GermanySerbia2010...\n",
      "Loading ./eval_tweets/GreeceIvoryCoast44.csv for game GreeceIvoryCoast44...\n",
      "Loading ./eval_tweets/GermanyGhana32.csv for game GermanyGhana32...\n",
      "Train Games: ['MexicoCroatia37', 'AustraliaSpain34', 'GermanyBrazil74', 'GermanyUSA57', 'FranceGermany70', 'PortugalGhana58', 'CameroonBrazil36', 'NetherlandsChile35', 'BelgiumSouthKorea59', 'USASlovenia2010', 'AustraliaNetherlands29', 'ArgentinaBelgium72', 'HondurasSwitzerland54', 'FranceNigeria66', 'ArgentinaGermanyFinal77', 'GermanyAlgeria67']\n",
      "Eval Games: ['NetherlandsMexico64', 'GermanySerbia2010', 'GreeceIvoryCoast44', 'GermanyGhana32']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_data_grouped(data_dir):\n",
    "    \"\"\"\n",
    "    Load all CSV files from the given directory, grouself.self.self.self.self.self.ped by game.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Path to the directory containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with keys `train` and `eval`, each containing a dictionary of DataFrames, \n",
    "              where each key is the game name.\n",
    "    \"\"\"\n",
    "    data = {\"train\": {}, \"eval\": {}}\n",
    "\n",
    "    for category in data.keys():\n",
    "        dir_path = os.path.join(data_dir, f\"{category}_tweets\")\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\") and not any(ext in file for ext in [\":Zone.Identifier\", \"_plots\"]):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    game_name = os.path.splitext(file)[0]  # Extract game name from file\n",
    "                    print(f\"Loading {file_path} for game {game_name}...\")\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    data[category][game_name] = df\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Usage example\n",
    "data_dir = \"./\"  # Replace with your dataset root directory\n",
    "data_grouped = load_data_grouped(data_dir)\n",
    "\n",
    "print(f\"Train Games: {list(data_grouped['train'].keys())}\")\n",
    "print(f\"Eval Games: {list(data_grouped['eval'].keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'MexicoCroatia37':             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0         19_0       19         0          0  1403553000000   \n",
       "  1         19_0       19         0          0  1403553000000   \n",
       "  2         19_0       19         0          0  1403553000000   \n",
       "  3         19_0       19         0          0  1403553000000   \n",
       "  4         19_0       19         0          0  1403553000000   \n",
       "  ...        ...      ...       ...        ...            ...   \n",
       "  155544  19_129       19       129          1  1403560800000   \n",
       "  155545  19_129       19       129          1  1403560800000   \n",
       "  155546  19_129       19       129          1  1403560800000   \n",
       "  155547  19_129       19       129          1  1403560800000   \n",
       "  155548  19_129       19       129          1  1403560800000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0       I have thee cutest Croatian following me (@Lar...  \n",
       "  1       RT @worldsoccershop: If @MarioMandzukic9 score...  \n",
       "  2       RT @worldsoccershop: If #Herrera scores for #M...  \n",
       "  3       RT @worldsoccershop: If @MarioMandzukic9 score...  \n",
       "  4       RT @worldsoccershop: If #Herrera scores for #M...  \n",
       "  ...                                                   ...  \n",
       "  155544  FINAL GROUP A STANDINGS:\\n#BRA 7 pts (+5 GD)\\n...  \n",
       "  155545  RT @FIFAWorldCup: GROUP A #WORLDCUP RESULTS:\\n...  \n",
       "  155546  RT @PurelyFootball: World Cup 2014 so far: \\n\\...  \n",
       "  155547  RT @FootballFunnys: World Cup 2014 so far: \\n\\...  \n",
       "  155548  Don't know what happened to my other tweet but...  \n",
       "  \n",
       "  [155549 rows x 6 columns],\n",
       "  'AustraliaSpain34':           ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0        2_0        2         0          0  1403538600000   \n",
       "  1        2_0        2         0          0  1403538600000   \n",
       "  2        2_0        2         0          0  1403538600000   \n",
       "  3        2_0        2         0          0  1403538600000   \n",
       "  4        2_0        2         0          0  1403538600000   \n",
       "  ...      ...      ...       ...        ...            ...   \n",
       "  86838  2_129        2       129          1  1403546400000   \n",
       "  86839  2_129        2       129          1  1403546400000   \n",
       "  86840  2_129        2       129          1  1403546400000   \n",
       "  86841  2_129        2       129          1  1403546400000   \n",
       "  86842  2_129        2       129          1  1403546400000   \n",
       "  \n",
       "                                                     Tweet  \n",
       "  0      RT @soccerdotcom: If #ESP beats #AUS we'll giv...  \n",
       "  1      Visit the #SITEP official web site here http:/...  \n",
       "  2      RT @soccerdotcom: If #ESP beats #AUS we'll giv...  \n",
       "  3      RT @worldsoccershop: If there is a winner in t...  \n",
       "  4      RT @soccerdotcom: If #AUS beats #ESP we'll giv...  \n",
       "  ...                                                  ...  \n",
       "  86838  RT @soccerdotcom: FINAL: #AUS 0-3 #ESP. Spain ...  \n",
       "  86839  RT @FIFAWorldCup: FT: #AUS 0-3 #ESP @Guaje7Vil...  \n",
       "  86840  Its end of the road for both #AUS and #ESP! Ca...  \n",
       "  86841  RT @FootballFact101: #WorldCup2014 Group B fin...  \n",
       "  86842  RT @EASPORTSFIFA: #NED tops Group B with a per...  \n",
       "  \n",
       "  [86843 rows x 6 columns],\n",
       "  'GermanyBrazil74':            ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0         1_0        1         0          0  1404849000000   \n",
       "  1         1_0        1         0          0  1404849000000   \n",
       "  2         1_0        1         0          0  1404849000000   \n",
       "  3         1_0        1         0          0  1404849000000   \n",
       "  4         1_0        1         0          0  1404849000000   \n",
       "  ...       ...      ...       ...        ...            ...   \n",
       "  973980  1_129        1       129          0  1404856800000   \n",
       "  973981  1_129        1       129          0  1404856800000   \n",
       "  973982  1_129        1       129          0  1404856800000   \n",
       "  973983  1_129        1       129          0  1404856800000   \n",
       "  973984  1_129        1       129          0  1404856800000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0       RT @RMadridHome: Sami Khedira is starting for ...  \n",
       "  1       RT @Yohanyx3: Damn half of ya saying go brazil...  \n",
       "  2       Colombia should be the ones playing against Ge...  \n",
       "  3       RT @tonyrizk_963: The Injury list :\\nFrom #BRA...  \n",
       "  4       Brazil should still be favored vs. Germanyâ€”if ...  \n",
       "  ...                                                   ...  \n",
       "  973980  RT @Arsenal: Congratulations to @MesutOzil1088...  \n",
       "  973981  Well Germany have no chance of winning the thi...  \n",
       "  973982  RT @SportsCenter: That's the first time Brazil...  \n",
       "  973983              Germany dominates f1 and football now  \n",
       "  973984  RT @TheAwayEnd: This is probably how Scotland ...  \n",
       "  \n",
       "  [973985 rows x 6 columns],\n",
       "  'GermanyUSA57':             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0         17_0       17         0          0  1403797800000   \n",
       "  1         17_0       17         0          0  1403797800000   \n",
       "  2         17_0       17         0          0  1403797800000   \n",
       "  3         17_0       17         0          0  1403797800000   \n",
       "  4         17_0       17         0          0  1403797800000   \n",
       "  ...        ...      ...       ...        ...            ...   \n",
       "  256440  17_129       17       129          1  1403805600000   \n",
       "  256441  17_129       17       129          1  1403805600000   \n",
       "  256442  17_129       17       129          1  1403805600000   \n",
       "  256443  17_129       17       129          1  1403805600000   \n",
       "  256444  17_129       17       129          1  1403805600000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0       @Gucci_waley  #worldcup #POR vs #GHA live in h...  \n",
       "  1       RT @Deadspin: Your live streaming links for Un...  \n",
       "  2       WORLD CUP 2014\\n#SWC14\\n\\nMatch 45: Thursday, ...  \n",
       "  3       RT @SportsCenter: #USA advances with a win or ...  \n",
       "  4       Coming up #WorldCup2014 \\nGroup G \\n#USA vs #G...  \n",
       "  ...                                                   ...  \n",
       "  256440  RT @BBCSport: Portugal fourth team in top 10 o...  \n",
       "  256441  RT @NBCSports: USA MOVES ON! Germany beats #US...  \n",
       "  256442  Ronaldo could have easily scored 4-5 goals ton...  \n",
       "  256443  RT @TheSelenatorBoy: Ppl getting mad bc Pepe i...  \n",
       "  256444  We grew game after game so we won this one. Al...  \n",
       "  \n",
       "  [256445 rows x 6 columns],\n",
       "  'FranceGermany70':            ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0         5_0        5         0          0  1404489000000   \n",
       "  1         5_0        5         0          0  1404489000000   \n",
       "  2         5_0        5         0          0  1404489000000   \n",
       "  3         5_0        5         0          0  1404489000000   \n",
       "  4         5_0        5         0          0  1404489000000   \n",
       "  ...       ...      ...       ...        ...            ...   \n",
       "  525720  5_129        5       129          1  1404496800000   \n",
       "  525721  5_129        5       129          1  1404496800000   \n",
       "  525722  5_129        5       129          1  1404496800000   \n",
       "  525723  5_129        5       129          1  1404496800000   \n",
       "  525724  5_129        5       129          1  1404496800000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0                        RT @julyhah: Go for it #GER ðŸ‡©ðŸ‡ªðŸ’ªðŸ˜„  \n",
       "  1       Me: mum who you supporting?\\n\\nMum: No one\\n\\n...  \n",
       "  2       RT @FootyQuandary: Who will win tonight? Your ...  \n",
       "  3       Germany vs France #GoogleDoodle\\nhttps://t.co/...  \n",
       "  4       Lookin' forward to #FRAGER goooo Germany!! #sc...  \n",
       "  ...                                                   ...  \n",
       "  525720  RT @OptaJoe: 4 - Germany are the first team in...  \n",
       "  525721  Watching back through old world cups where Ger...  \n",
       "  525722  RT @chalkontheboots: If France lose, it's beca...  \n",
       "  525723  Vinny, Jack Wilshere just asked why the best c...  \n",
       "  525724  #SSFootball France fucked up by not starting L...  \n",
       "  \n",
       "  [525725 rows x 6 columns],\n",
       "  'PortugalGhana58':            ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0         3_0        3         0          0  1403797800000   \n",
       "  1         3_0        3         0          0  1403797800000   \n",
       "  2         3_0        3         0          0  1403797800000   \n",
       "  3         3_0        3         0          0  1403797800000   \n",
       "  4         3_0        3         0          0  1403797800000   \n",
       "  ...       ...      ...       ...        ...            ...   \n",
       "  272384  3_129        3       129          1  1403805600000   \n",
       "  272385  3_129        3       129          1  1403805600000   \n",
       "  272386  3_129        3       129          1  1403805600000   \n",
       "  272387  3_129        3       129          1  1403805600000   \n",
       "  272388  3_129        3       129          1  1403805600000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0       @Gucci_waley  #worldcup #POR vs #GHA live in h...  \n",
       "  1       RT @Deadspin: Your live streaming links for Un...  \n",
       "  2       WORLD CUP 2014\\n#SWC14\\n\\nMatch 45: Thursday, ...  \n",
       "  3       RT @SportsCenter: #USA advances with a win or ...  \n",
       "  4       Cried a little....the USA and Ghana matches ar...  \n",
       "  ...                                                   ...  \n",
       "  272384  RT @RealEsparta: #POR 2-1 #GHA: Cristiano Rona...  \n",
       "  272385  RT @NBCSports: USA MOVES ON! Germany beats #US...  \n",
       "  272386  RT @xTrustAndObey: Folks don't rate Ghana. smh...  \n",
       "  272387  Ronaldo could have easily scored 4-5 goals ton...  \n",
       "  272388  @dubroland the United States would not have pa...  \n",
       "  \n",
       "  [272389 rows x 6 columns],\n",
       "  'CameroonBrazil36':            ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0         8_0        8         0          0  1403553000000   \n",
       "  1         8_0        8         0          0  1403553000000   \n",
       "  2         8_0        8         0          0  1403553000000   \n",
       "  3         8_0        8         0          0  1403553000000   \n",
       "  4         8_0        8         0          0  1403553000000   \n",
       "  ...       ...      ...       ...        ...            ...   \n",
       "  148293  8_129        8       129          1  1403560800000   \n",
       "  148294  8_129        8       129          1  1403560800000   \n",
       "  148295  8_129        8       129          1  1403560800000   \n",
       "  148296  8_129        8       129          1  1403560800000   \n",
       "  148297  8_129        8       129          1  1403560800000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0       RT @Oddschanger: #CMR v #BRA\\n\\nIf OSCAR score...  \n",
       "  1       RT @soccerdotcom: If @neymarjr scores vs #CMR ...  \n",
       "  2       RT @worldsoccershop: If there is a winner in t...  \n",
       "  3       RT @worldsoccershop: If there is a winner in t...  \n",
       "  4       RT @soccerdotcom: If @neymarjr scores vs #CMR ...  \n",
       "  ...                                                   ...  \n",
       "  148293  RT @FIFAWorldCup: FINAL GROUP A STANDINGS:\\n#B...  \n",
       "  148294  FINAL GROUP A STANDINGS:\\n#BRA 7 pts (+5 GD)\\n...  \n",
       "  148295  RT @FIFAWorldCup: GROUP A #WORLDCUP RESULTS:\\n...  \n",
       "  148296  RT @PurelyFootball: World Cup 2014 so far: \\n\\...  \n",
       "  148297  RT @FootballFunnys: World Cup 2014 so far: \\n\\...  \n",
       "  \n",
       "  [148298 rows x 6 columns],\n",
       "  'NetherlandsChile35':           ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0        7_0        7         0          0  1403538600000   \n",
       "  1        7_0        7         0          0  1403538600000   \n",
       "  2        7_0        7         0          0  1403538600000   \n",
       "  3        7_0        7         0          0  1403538600000   \n",
       "  4        7_0        7         0          0  1403538600000   \n",
       "  ...      ...      ...       ...        ...            ...   \n",
       "  95103  7_129        7       129          1  1403546400000   \n",
       "  95104  7_129        7       129          1  1403546400000   \n",
       "  95105  7_129        7       129          1  1403546400000   \n",
       "  95106  7_129        7       129          1  1403546400000   \n",
       "  95107  7_129        7       129          1  1403546400000   \n",
       "  \n",
       "                                                     Tweet  \n",
       "  0      Watch Ned vs Chi 2014 WC Match Online in HD ht...  \n",
       "  1      RT @SoccerrUniverse: If #Sanchez scores agains...  \n",
       "  2      RT @soccerdotcom: If @sneijder101010 scores vs...  \n",
       "  3      RT @soccerdotcom: If @sneijder101010 scores vs...  \n",
       "  4      RT @martinezyayoo: what I'm looking forward to...  \n",
       "  ...                                                  ...  \n",
       "  95103  RT @ManUtd: Van Gaal's side need just a point ...  \n",
       "  95104  RT @Bolanet: #PiDun2014 - FT: #NED 2-0 #CHI (L...  \n",
       "  95105  RT @FootballFact101: #WorldCup2014 Group B fin...  \n",
       "  95106  RT @ManUtd: Louis van Gaal has masterminded an...  \n",
       "  95107  RT @EASPORTSFIFA: #NED tops Group B with a per...  \n",
       "  \n",
       "  [95108 rows x 6 columns],\n",
       "  'BelgiumSouthKorea59':            ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0        14_0       14         0          0  1403812200000   \n",
       "  1        14_0       14         0          0  1403812200000   \n",
       "  2        14_0       14         0          0  1403812200000   \n",
       "  3        14_0       14         0          0  1403812200000   \n",
       "  4        14_0       14         0          0  1403812200000   \n",
       "  ...       ...      ...       ...        ...            ...   \n",
       "  99187  14_129       14       129          1  1403820000000   \n",
       "  99188  14_129       14       129          1  1403820000000   \n",
       "  99189  14_129       14       129          1  1403820000000   \n",
       "  99190  14_129       14       129          1  1403820000000   \n",
       "  99191  14_129       14       129          1  1403820000000   \n",
       "  \n",
       "                                                     Tweet  \n",
       "  0      RT @FutballTweets: If #Fellaini #BEL scores fi...  \n",
       "  1      RT @FCsoccerplayers: If Belgium win we will gi...  \n",
       "  2      Serious bruh!\"@Sparks051: Belgium hasn't reall...  \n",
       "  3      RT @mayitoab: {1} 140626  #Minho in Brazil to ...  \n",
       "  4      U.S. advances, Man Utd gets Herrera and Shaw, ...  \n",
       "  ...                                                  ...  \n",
       "  99187  Belgium? Are they in the National League West ...  \n",
       "  99188  RT @JuanPaGalavis: USA has a GREAT chance agai...  \n",
       "  99189                       Belgium about to fuck USA up  \n",
       "  99190  RT @soccerdotcom: The #WorldCup Round of 16!\\n...  \n",
       "  99191                 Belgium it is, we got this. #USMNT  \n",
       "  \n",
       "  [99192 rows x 6 columns],\n",
       "  'USASlovenia2010':            ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0        18_0       18         0          0  1276869000000   \n",
       "  1        18_0       18         0          0  1276869000000   \n",
       "  2        18_0       18         0          0  1276869000000   \n",
       "  3        18_0       18         0          0  1276869000000   \n",
       "  4        18_0       18         0          0  1276869000000   \n",
       "  ...       ...      ...       ...        ...            ...   \n",
       "  85670  18_129       18       129          0  1276876799000   \n",
       "  85671  18_129       18       129          0  1276876799000   \n",
       "  85672  18_129       18       129          0  1276876799000   \n",
       "  85673  18_129       18       129          0  1276876799000   \n",
       "  85674  18_129       18       129          0  1276876799000   \n",
       "  \n",
       "                                                     Tweet  \n",
       "  0      #USA All My Stateside Followers Stand Up And R...  \n",
       "  1      @Lynz_89 I think the ref might have been Basil...  \n",
       "  2      Hoping a #USA win can help ease the pain of la...  \n",
       "  3              When does this actually start?  #worldcup  \n",
       "  4      Hanson and Roy are a proper pundit line up. #w...  \n",
       "  ...                                                  ...  \n",
       "  85670  RT @nytimes FIFA World Cup -- Final Score: U.S...  \n",
       "  85671          Ugh!!! should've been 3-2 USA!  #worldcup  \n",
       "  85672  RT @jaclynkeough: Ha! RT @someecards I'd rathe...  \n",
       "  85673  RT @gustavaulia: So many surprises at worldcup...  \n",
       "  85674  RT @themotleyfool: Also, Team #USA got robbed....  \n",
       "  \n",
       "  [85675 rows x 6 columns],\n",
       "  'AustraliaNetherlands29':           ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0       12_0       12         0          0  1403106600000   \n",
       "  1       12_0       12         0          0  1403106600000   \n",
       "  2       12_0       12         0          0  1403106600000   \n",
       "  3       12_0       12         0          0  1403106600000   \n",
       "  4       12_0       12         0          0  1403106600000   \n",
       "  ...      ...      ...       ...        ...            ...   \n",
       "  96829  12_96       12        96          1  1403112405000   \n",
       "  96830  12_96       12        96          1  1403112405000   \n",
       "  96831  12_96       12        96          1  1403112405000   \n",
       "  96832  12_96       12        96          1  1403112405000   \n",
       "  96833  12_96       12        96          1  1403112405000   \n",
       "  \n",
       "                                                     Tweet  \n",
       "  0      RT @FIFAWorldCup: #AUS LINE-UP: Ryan, Davidson...  \n",
       "  1      â€œ@WSJSports: #USA soccer's Clint Dempsey, a.k....  \n",
       "  2      #persieing in his sleep! #AUS #NED #WorldCup20...  \n",
       "  3      Glad I got my tune in radio to listen to the D...  \n",
       "  4      INSTAGRAM: Are you following the #WorldCup @in...  \n",
       "  ...                                                  ...  \n",
       "  96829  rt @FIFAWorldCup GOAL: #AUS 2-3 #NED The Dutch...  \n",
       "  96830  Beat team providing best entertainment and gua...  \n",
       "  96831  RT @shauntrott88: This Australia Vs Holland ma...  \n",
       "  96832  RT @FIFAWorldCup: GOAL: #AUS 2-3 #NED The Dutc...  \n",
       "  96833  RT @FIFAWorldCup: GOAL: #AUS 2-3 #NED The Dutc...  \n",
       "  \n",
       "  [96834 rows x 6 columns],\n",
       "  'ArgentinaBelgium72':             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0         11_0       11         0          0  1404575400000   \n",
       "  1         11_0       11         0          0  1404575400000   \n",
       "  2         11_0       11         0          0  1404575400000   \n",
       "  3         11_0       11         0          0  1404575400000   \n",
       "  4         11_0       11         0          0  1404575400000   \n",
       "  ...        ...      ...       ...        ...            ...   \n",
       "  313798  11_129       11       129          0  1404583200000   \n",
       "  313799  11_129       11       129          0  1404583200000   \n",
       "  313800  11_129       11       129          0  1404583200000   \n",
       "  313801  11_129       11       129          0  1404583200000   \n",
       "  313802  11_129       11       129          0  1404583200000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0       RT @2014WorIdCup: Argentina vs Belgium\\n\\nWho ...  \n",
       "  1       @elijahman_ time to focus on Belgium winning t...  \n",
       "  2       RT @FIFAWorldCup: GLOBAL STADIUM: #Joinin with...  \n",
       "  3       RT @CatholicNewsSvc: #PopeFrancis. Uh-oh. Arge...  \n",
       "  4       RT @soccerdotcom: If he scores vs #BEL we'll a...  \n",
       "  ...                                                   ...  \n",
       "  313798  RT @2014WC_Brazil: This is the first time in W...  \n",
       "  313799  RT @Footy___Girls: Lovely Celebration  #ARG\\n ...  \n",
       "  313800  It's not like I hate the country or anything, ...  \n",
       "  313801  RT @UtdIndonesiaBDG: Yah gugur, gutted couldn'...  \n",
       "  313802                 Can't wait for Argentina to go out  \n",
       "  \n",
       "  [313803 rows x 6 columns],\n",
       "  'HondurasSwitzerland54':           ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0        0_0        0         0          0  1403725800000   \n",
       "  1        0_0        0         0          0  1403725800000   \n",
       "  2        0_0        0         0          0  1403725801000   \n",
       "  3        0_0        0         0          0  1403725802000   \n",
       "  4        0_0        0         0          0  1403725802000   \n",
       "  ...      ...      ...       ...        ...            ...   \n",
       "  41534  0_129        0       129          1  1403733600000   \n",
       "  41535  0_129        0       129          1  1403733600000   \n",
       "  41536  0_129        0       129          1  1403733600000   \n",
       "  41537  0_129        0       129          1  1403733600000   \n",
       "  41538  0_129        0       129          1  1403733600000   \n",
       "  \n",
       "                                                     Tweet  \n",
       "  0      RT @soccerdotcom: Pick a #WorldCup Side! \\nRet...  \n",
       "  1      RT @FOXSoccer: #WorldCup Group E standings:\\n\\...  \n",
       "  2      #honduras vs #ecuador - current tweets: 852:27...  \n",
       "  3      Ecuador have not picked up any injuries and th...  \n",
       "  4      RT @TheSCRLife: If #HON wins weâ€™ll give away a...  \n",
       "  ...                                                  ...  \n",
       "  41534  RT @FIFAWorldCup: FT: #HON 0-3 #SUI Switzerlan...  \n",
       "  41535  World - FIFA World Cup - Group Stage < Hondura...  \n",
       "  41536  RT @Bundesliga_EN: Another @FCBayernEN hat-tri...  \n",
       "  41537  Switzerland beat Honduras 3-0 in a World Cup G...  \n",
       "  41538  RT @FIFAWorldCup: FT: #HON 0-3 #SUI Switzerlan...  \n",
       "  \n",
       "  [41539 rows x 6 columns],\n",
       "  'FranceNigeria66':             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0         13_0       13         0          0  1404143400000   \n",
       "  1         13_0       13         0          0  1404143400000   \n",
       "  2         13_0       13         0          0  1404143400000   \n",
       "  3         13_0       13         0          0  1404143400000   \n",
       "  4         13_0       13         0          0  1404143400000   \n",
       "  ...        ...      ...       ...        ...            ...   \n",
       "  367894  13_129       13       129          0  1404151200000   \n",
       "  367895  13_129       13       129          0  1404151200000   \n",
       "  367896  13_129       13       129          0  1404151200000   \n",
       "  367897  13_129       13       129          0  1404151200000   \n",
       "  367898  13_129       13       129          0  1404151200000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0       Team #NGA ........you may just be eleven indiv...  \n",
       "  1                               France have some mad team  \n",
       "  2       RT @FOXSoccer: Real Madrid or Barcelona? \\n\\n#...  \n",
       "  3                     France will win today's first match  \n",
       "  4       @habib_habibug @Ameenuo nothing for una today....  \n",
       "  ...                                                   ...  \n",
       "  367894  RT @SirUTI: NONE DARE INSULT THE SUPER EAGLES ...  \n",
       "  367895  RT @juventusfcen: A @paulpogba header sets #FR...  \n",
       "  367896                                 God job France (y)  \n",
       "  367897  RT @NGSuperEagles: FT: #FRA 2-0 #NGA. And so o...  \n",
       "  367898  Now is up to Germany to beat Algeria and knock...  \n",
       "  \n",
       "  [367899 rows x 6 columns],\n",
       "  'ArgentinaGermanyFinal77':             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0         10_0       10         0          0  1405277400000   \n",
       "  1         10_0       10         0          0  1405277400000   \n",
       "  2         10_0       10         0          0  1405277400000   \n",
       "  3         10_0       10         0          0  1405277400000   \n",
       "  4         10_0       10         0          0  1405277400000   \n",
       "  ...        ...      ...       ...        ...            ...   \n",
       "  824236  10_179       10       179          1  1405288200000   \n",
       "  824237  10_179       10       179          1  1405288200000   \n",
       "  824238  10_179       10       179          1  1405288200000   \n",
       "  824239  10_179       10       179          1  1405288200000   \n",
       "  824240  10_179       10       179          1  1405288200000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0       RT @ESPNFC: INJURY UPDATE: Khedira (calf) is r...  \n",
       "  1       Where Does Brazil's 2014 World Cup Campaign Ra...  \n",
       "  2                               #ARG for the world cup!!!  \n",
       "  3       RT @Notiice: RT for #GER \\nFAV for #ARG \\n\\nI'...  \n",
       "  4       Every Mexican is rooting for Argentina \\nSo I ...  \n",
       "  ...                                                   ...  \n",
       "  824236  i bet the Argentina players will get nightmare...  \n",
       "  824237  U have no idea about the TURN UP that is about...  \n",
       "  824238  @SvenvanderSteen zeeeeee argies beat us sooooo...  \n",
       "  824239  RT @teewhizzle: Germany needs 2 goan learn how...  \n",
       "  824240  RT @SonySIX: FANTASTIC GOAL BY GOTZE!! Definit...  \n",
       "  \n",
       "  [824241 rows x 6 columns],\n",
       "  'GermanyAlgeria67':            ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       "  0         4_0        4         0          0  1404157800000   \n",
       "  1         4_0        4         0          0  1404157800000   \n",
       "  2         4_0        4         0          0  1404157800000   \n",
       "  3         4_0        4         0          0  1404157800000   \n",
       "  4         4_0        4         0          0  1404157800000   \n",
       "  ...       ...      ...       ...        ...            ...   \n",
       "  712520  4_169        4       169          0  1404168000000   \n",
       "  712521  4_169        4       169          0  1404168000000   \n",
       "  712522  4_169        4       169          0  1404168000000   \n",
       "  712523  4_169        4       169          0  1404168000000   \n",
       "  712524  4_169        4       169          0  1404168000000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0       RT @FOXSoccer: 2006 World Cup: 147 goals TOTAL...  \n",
       "  1       RT @TransferSources: GIVEAWAY: If Ozil scores ...  \n",
       "  2       I don mind riding alone \"@Not_anIdiot: Haha le...  \n",
       "  3       RT @dw_sports: No current @bvb players in the ...  \n",
       "  4       RT @trueSCRlife: If MÃ¼ller scores for #GER we'...  \n",
       "  ...                                                   ...  \n",
       "  712520  RT @FOXSoccer: 3/4 of the #WorldCup quarterfin...  \n",
       "  712521  RT @Rodolph_hilal: Plz guys RETWEET .. \\n\\nLet...  \n",
       "  712522  RT @Joey7Barton: Algeria can take a lot of pos...  \n",
       "  712523  RT @caughtoffside: #ALG gave it their all, was...  \n",
       "  712524  So be it...  #FRA #GER in 1/4 finals ^_^ #worl...  \n",
       "  \n",
       "  [712525 rows x 6 columns]},\n",
       " 'eval': {'NetherlandsMexico64':             ID  MatchID  PeriodID      Timestamp  \\\n",
       "  0         15_0       15         0  1404057289000   \n",
       "  1         15_0       15         0  1404057289000   \n",
       "  2         15_0       15         0  1404057289000   \n",
       "  3         15_0       15         0  1404057289000   \n",
       "  4         15_0       15         0  1404057289000   \n",
       "  ...        ...      ...       ...            ...   \n",
       "  628693  15_125       15       125  1404064800000   \n",
       "  628694  15_125       15       125  1404064800000   \n",
       "  628695  15_125       15       125  1404064800000   \n",
       "  628696  15_125       15       125  1404064800000   \n",
       "  628697  15_125       15       125  1404064800000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0       I feel like shit >.< but gonna watch the game ...  \n",
       "  1       As much as I like Herrera and Ochoa (because h...  \n",
       "  2       RT @lfc_family: If Robben scores for #NED we'l...  \n",
       "  3       Mexico Game with Benâš½ï¸âš½ï¸ @ Broadacres Marketpl...  \n",
       "  4       RT @TrueSoccerLife: If #MEX wins we'll give aw...  \n",
       "  ...                                                   ...  \n",
       "  628693  Dutch deserve to be in last 8.Keep their nerve...  \n",
       "  628694  RT @GeniusFootball: RETWEET if you think #MEX ...  \n",
       "  628695  Hold your head high Mexico, played beautifully...  \n",
       "  628696  RT @TheWorIdCup: Mexico fans right now... http...  \n",
       "  628697  Go @FEDEFUTBOL_CR! #WorldCup Who are you suppo...  \n",
       "  \n",
       "  [628698 rows x 5 columns],\n",
       "  'GermanySerbia2010':            ID  MatchID  PeriodID      Timestamp  \\\n",
       "  0        16_0       16         0  1276860000000   \n",
       "  1        16_0       16         0  1276860001000   \n",
       "  2        16_0       16         0  1276860002000   \n",
       "  3        16_0       16         0  1276860002000   \n",
       "  4        16_0       16         0  1276860002000   \n",
       "  ...       ...      ...       ...            ...   \n",
       "  45019  16_129       16       129  1276867800000   \n",
       "  45020  16_129       16       129  1276867800000   \n",
       "  45021  16_129       16       129  1276867800000   \n",
       "  45022  16_129       16       129  1276867800000   \n",
       "  45023  16_129       16       129  1276867800000   \n",
       "  \n",
       "                                                     Tweet  \n",
       "  0      RT @CastrolGOAL    Who wants to win a Castrol ...  \n",
       "  1      2010 FIFA World Cup latest >> http://shrten.me...  \n",
       "  2      #England Wallpaper for #iPhone / #iPod, now av...  \n",
       "  3      #worldcup Germany have been very impressive so...  \n",
       "  4      #wc2010 ~ The england twibbon is doing great, ...  \n",
       "  ...                                                  ...  \n",
       "  45019                             LETS GO #USA #worldcup  \n",
       "  45020  another upset in #WC2010 #Srb beat #Ger by 1-0 !!  \n",
       "  45021  RT @FIFAcom: #GER 0:1 #SRB: TheÃ‚Â finalÃ‚Â whistl...  \n",
       "  45022  dukung yg menang -_- RT @AlikaZahira: #bra #fr...  \n",
       "  45023          Serbia win?wow...unexpected,hm? #worldcup  \n",
       "  \n",
       "  [45024 rows x 5 columns],\n",
       "  'GreeceIvoryCoast44':            ID  MatchID  PeriodID      Timestamp  \\\n",
       "  0         9_0        9         0  1403639400000   \n",
       "  1         9_0        9         0  1403639400000   \n",
       "  2         9_0        9         0  1403639400000   \n",
       "  3         9_0        9         0  1403639400000   \n",
       "  4         9_0        9         0  1403639400000   \n",
       "  ...       ...      ...       ...            ...   \n",
       "  113397  9_129        9       129  1403647200000   \n",
       "  113398  9_129        9       129  1403647200000   \n",
       "  113399  9_129        9       129  1403647200000   \n",
       "  113400  9_129        9       129  1403647200000   \n",
       "  113401  9_129        9       129  1403647200000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0       Wana place a bet on an ivory coast win but ain...  \n",
       "  1       #WatchLive @FIFAWorldCup: Greece vs. Ivory Coa...  \n",
       "  2       Gonna watch the Colombia and Japan game becaus...  \n",
       "  3       #CIV vs #COL & #GRE vs #JPN! It would be inter...  \n",
       "  4       RT @DuncanCastles: Quite a statistic this: Gre...  \n",
       "  ...                                                   ...  \n",
       "  113397  RT @xja_mila: Ivory coast didnt fucking deserv...  \n",
       "  113398  If you lose because of poor conditioning and l...  \n",
       "  113399  Oh dear @bobbykemp81, Spain, Italy,Bosnia & Iv...  \n",
       "  113400  Exactly \"@Naijablogger: Ivory Coast Was So clo...  \n",
       "  113401                   RT @Gonth93: IVORY SHIT ON TOAST  \n",
       "  \n",
       "  [113402 rows x 5 columns],\n",
       "  'GermanyGhana32':            ID  MatchID  PeriodID      Timestamp  \\\n",
       "  0         6_0        6         0  1403376600000   \n",
       "  1         6_0        6         0  1403376600000   \n",
       "  2         6_0        6         0  1403376600000   \n",
       "  3         6_0        6         0  1403376600000   \n",
       "  4         6_0        6         0  1403376600000   \n",
       "  ...       ...      ...       ...            ...   \n",
       "  285799  6_129        6       129  1403384400000   \n",
       "  285800  6_129        6       129  1403384400000   \n",
       "  285801  6_129        6       129  1403384400000   \n",
       "  285802  6_129        6       129  1403384400000   \n",
       "  285803  6_129        6       129  1403384400000   \n",
       "  \n",
       "                                                      Tweet  \n",
       "  0          I Finally get to see Germany play\\n#GER   ðŸ‡©ðŸ‡ªâš½ðŸ†  \n",
       "  1       RT @Wor1dCup2014: If Any of the Boateng Brothe...  \n",
       "  2       Fascinated for this #GERvsGHA match. This will...  \n",
       "  3                               : #GER and #GHA in a few.  \n",
       "  4       BOATENG GRUDGE MATCH: 21/2 for Jermaine to sco...  \n",
       "  ...                                                   ...  \n",
       "  285799  Germany is in a good spot, I am not worried. W...  \n",
       "  285800  RT @World: FT: #GER 2-2 #GHA -- What a brillia...  \n",
       "  285801  RT @TaylorTwellman: If #GHA finished their cou...  \n",
       "  285802  RT @SportsmediaUK: Germany and Ghana drew 2-2 ...  \n",
       "  285803  RT @soccerdotcom: FINAL: #GER 2-2 #GHA Incredi...  \n",
       "  \n",
       "  [285804 rows x 5 columns]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((313803, 6),\n",
       "      ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
       " 0  11_0       11         0          0  1404575400000   \n",
       " 1  11_0       11         0          0  1404575400000   \n",
       " 2  11_0       11         0          0  1404575400000   \n",
       " 3  11_0       11         0          0  1404575400000   \n",
       " 4  11_0       11         0          0  1404575400000   \n",
       " \n",
       "                                                Tweet  \n",
       " 0  RT @2014WorIdCup: Argentina vs Belgium\\n\\nWho ...  \n",
       " 1  @elijahman_ time to focus on Belgium winning t...  \n",
       " 2  RT @FIFAWorldCup: GLOBAL STADIUM: #Joinin with...  \n",
       " 3  RT @CatholicNewsSvc: #PopeFrancis. Uh-oh. Arge...  \n",
       " 4  RT @soccerdotcom: If he scores vs #BEL we'll a...  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_grouped['train'][\"ArgentinaBelgium72\"].shape, data_grouped['train'][\"ArgentinaBelgium72\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/elio_samaha/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 15:24:51.124896: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-05 15:24:51.349364: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-05 15:24:51.349399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-05 15:24:51.404707: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-05 15:24:51.484215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data for game: MexicoCroatia37\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: AustraliaSpain34\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: GermanyBrazil74\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: GermanyUSA57\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: FranceGermany70\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: PortugalGhana58\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: CameroonBrazil36\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: NetherlandsChile35\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: BelgiumSouthKorea59\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: USASlovenia2010\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: AustraliaNetherlands29\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: ArgentinaBelgium72\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: HondurasSwitzerland54\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: FranceNigeria66\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: ArgentinaGermanyFinal77\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: GermanyAlgeria67\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: NetherlandsMexico64\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: GermanySerbia2010\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: GreeceIvoryCoast44\n",
      "Cleaning tweets...\n",
      "Cleaning data for game: GermanyGhana32\n",
      "Cleaning tweets...\n"
     ]
    }
   ],
   "source": [
    "from utils import preprocess_text  # Use or enhance the existing function\n",
    "\n",
    "def clean_tweets(df, remove_retweets=True):\n",
    "    \"\"\"\n",
    "    Clean a single DataFrame of tweets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing tweets.\n",
    "        remove_retweets (bool): Whether to remove retweets.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"Cleaning tweets...\")\n",
    "    if \"Tweet\" not in df.columns:\n",
    "        raise ValueError(\"The DataFrame must contain a 'Tweet' column.\")\n",
    "\n",
    "    # Remove retweets if necessary\n",
    "    if remove_retweets:\n",
    "        df = df[~df[\"Tweet\"].str.startswith(\"RT\")]\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=[\"Tweet\"])\n",
    "\n",
    "    # Apply preprocessing to text\n",
    "    df[\"cleaned_text\"] = df[\"Tweet\"].apply(preprocess_text)\n",
    "\n",
    "    # Drop very short tweets if desired\n",
    "    # df = df[df[\"cleaned_text\"].str.len() > 10]  # Drop tweets with <= 10 characters\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_data_grouped(data_grouped, remove_retweets=True):\n",
    "    \"\"\"\n",
    "    Clean the grouped dataset (grouped by games).\n",
    "\n",
    "    Args:\n",
    "        data_grouped (dict): Dictionary of DataFrames grouped by game.\n",
    "        remove_retweets (bool): Whether to remove retweets.\n",
    "\n",
    "    Returns:\n",
    "        dict: Cleaned grouped dataset.\n",
    "    \"\"\"\n",
    "    for category in data_grouped.keys():\n",
    "        for game, df in data_grouped[category].items():\n",
    "            print(f\"Cleaning data for game: {game}\")\n",
    "            data_grouped[category][game] = clean_tweets(df, remove_retweets=remove_retweets)\n",
    "    \n",
    "    return data_grouped\n",
    "\n",
    "# Usage\n",
    "data_grouped = clean_data_grouped(data_grouped, remove_retweets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from pickle file...\n",
      "Loaded and split data into 16 games.\n",
      "Game: 0, Number of Periods: 130\n",
      "                                aggregated_embedding  EventType MatchID\n",
      "0  [0.038894046, 0.2056955, 0.1732219, -0.0507800...          0       0\n",
      "1  [0.049072143, 0.20504853, 0.16913632, -0.04101...          0       0\n",
      "2  [-0.010199165, 0.19084008, 0.142129, -0.027013...          1       0\n",
      "3  [0.1076907, 0.18990463, 0.13662358, -0.0059914...          1       0\n",
      "4  [0.110753596, 0.21662608, 0.14855266, -0.02414...          1       0\n",
      "Game: 1, Number of Periods: 130\n",
      "                                   aggregated_embedding  EventType MatchID\n",
      "1187  [-0.02544716, 0.11435313, 0.106707424, -0.0815...          0       1\n",
      "1188  [-0.028089345, 0.11905627, 0.11491479, -0.0808...          0       1\n",
      "1189  [-0.035966076, 0.11795183, 0.12116859, -0.0814...          1       1\n",
      "1190  [-0.07086158, 0.16810267, 0.1163678, -0.077644...          1       1\n",
      "1191  [-0.073121004, 0.1698075, 0.11858397, -0.07978...          1       1\n",
      "Game: 10, Number of Periods: 180\n",
      "                                  aggregated_embedding  EventType MatchID\n",
      "130  [0.024309061, 0.17246658, 0.08133196, -0.08583...          0      10\n",
      "131  [0.027523618, 0.1734573, 0.0913784, -0.0863043...          0      10\n",
      "132  [0.029151151, 0.16438664, 0.089944065, -0.0838...          1      10\n",
      "133  [-0.0072107846, 0.17703997, 0.09721466, -0.078...          0      10\n",
      "134  [-0.0025906938, 0.17473103, 0.09527402, -0.076...          0      10\n",
      "Game: 11, Number of Periods: 130\n",
      "                                  aggregated_embedding  EventType MatchID\n",
      "310  [-0.0041261837, 0.16640277, 0.12733154, -0.055...          0      11\n",
      "311  [-0.0045492817, 0.16534437, 0.13236286, -0.055...          0      11\n",
      "312  [-0.0040970845, 0.16112202, 0.13261095, -0.055...          1      11\n",
      "313  [-0.031963814, 0.17230007, 0.09916434, -0.0487...          0      11\n",
      "314  [-0.02636523, 0.17290369, 0.109980114, -0.0526...          0      11\n",
      "Game: 12, Number of Periods: 97\n",
      "                                  aggregated_embedding  EventType MatchID\n",
      "440  [0.006067639, 0.17381522, 0.109002516, -0.0424...          0      12\n",
      "441  [0.0004618784, 0.18718727, 0.11162407, -0.0495...          0      12\n",
      "442  [0.030415297, 0.16079392, 0.13407741, -0.06066...          1      12\n",
      "443  [0.039453216, 0.14320451, 0.14097434, -0.06368...          1      12\n",
      "444  [0.016411506, 0.14968121, 0.13760446, -0.06413...          0      12\n",
      "Game: 13, Number of Periods: 130\n",
      "                                  aggregated_embedding  EventType MatchID\n",
      "537  [-0.022434248, 0.17517151, 0.14636572, -0.0691...          0      13\n",
      "538  [-0.02637213, 0.16887625, 0.14297616, -0.06755...          0      13\n",
      "539  [-0.030635461, 0.16003962, 0.1501157, -0.06853...          1      13\n",
      "540  [-0.030932417, 0.17964625, 0.14299671, -0.0545...          1      13\n",
      "541  [-0.031426135, 0.17772138, 0.14443557, -0.0490...          0      13\n",
      "Game: 14, Number of Periods: 130\n",
      "                                  aggregated_embedding  EventType MatchID\n",
      "667  [-0.10823773, 0.14829654, 0.13990103, -0.07365...          0      14\n",
      "668  [-0.1150199, 0.14382453, 0.14893939, -0.072792...          1      14\n",
      "669  [-0.11150841, 0.15311155, 0.17889683, -0.06999...          1      14\n",
      "670  [-0.11471102, 0.12889892, 0.14312282, -0.06352...          0      14\n",
      "671  [-0.10698167, 0.14086767, 0.147184, -0.0696160...          1      14\n",
      "Game: 17, Number of Periods: 130\n",
      "                                  aggregated_embedding  EventType MatchID\n",
      "797  [0.010528235, 0.18458702, 0.11788609, -0.04031...          0      17\n",
      "798  [0.00487689, 0.1780795, 0.11917785, -0.0447551...          0      17\n",
      "799  [-0.0045401035, 0.13580994, 0.120824255, -0.07...          1      17\n",
      "800  [0.014151361, 0.13488413, 0.15929745, -0.06366...          0      17\n",
      "801  [0.008974781, 0.13312963, 0.15924786, -0.06229...          1      17\n",
      "Game: 18, Number of Periods: 130\n",
      "                                  aggregated_embedding  EventType MatchID\n",
      "927  [-0.010390969, 0.16845895, 0.12628657, -0.0995...          0      18\n",
      "928  [-0.018066613, 0.17959581, 0.12504593, -0.1086...          0      18\n",
      "929  [-0.004151128, 0.16727214, 0.14765027, -0.0952...          0      18\n",
      "930  [-0.015806807, 0.16615726, 0.12413403, -0.0734...          0      18\n",
      "931  [-0.024141518, 0.17034523, 0.11712637, -0.0774...          0      18\n",
      "Game: 19, Number of Periods: 130\n",
      "                                   aggregated_embedding  EventType MatchID\n",
      "1057  [-0.056150448, 0.13037562, 0.06238931, 0.00472...          0      19\n",
      "1058  [-0.052992813, 0.12564498, 0.07133314, 0.00239...          0      19\n",
      "1059  [-0.064378984, 0.108784705, 0.0861132, 0.01687...          1      19\n",
      "1060  [0.006931626, 0.13041559, 0.024011046, 0.01650...          1      19\n",
      "1061  [-0.0043271687, 0.13214818, 0.025328986, 0.031...          1      19\n",
      "Game: 2, Number of Periods: 130\n",
      "                                   aggregated_embedding  EventType MatchID\n",
      "1317  [0.10314335, 0.1923181, 0.1437286, -0.01545752...          0       2\n",
      "1318  [0.10449707, 0.19663905, 0.14836986, -0.015874...          0       2\n",
      "1319  [0.11107254, 0.16460766, 0.13733025, -0.019084...          1       2\n",
      "1320  [0.0541511, 0.22091082, 0.06935183, 0.00181718...          1       2\n",
      "1321  [0.05132015, 0.22313993, 0.06788071, 0.0003350...          1       2\n",
      "Game: 3, Number of Periods: 130\n",
      "                                   aggregated_embedding  EventType MatchID\n",
      "1447  [0.025824025, 0.20216404, 0.11338887, -0.06735...          0       3\n",
      "1448  [0.017259426, 0.19788772, 0.11277955, -0.07379...          0       3\n",
      "1449  [0.024586003, 0.15926448, 0.1165276, -0.097266...          1       3\n",
      "1450  [0.024154438, 0.1971927, 0.099950016, -0.07526...          0       3\n",
      "1451  [0.024489312, 0.19751526, 0.101137064, -0.0743...          0       3\n",
      "Game: 4, Number of Periods: 170\n",
      "                                   aggregated_embedding  EventType MatchID\n",
      "1577  [0.0035258923, 0.17729685, 0.11139445, -0.0996...          0       4\n",
      "1578  [0.0038384115, 0.17716962, 0.111889414, -0.098...          0       4\n",
      "1579  [0.013819734, 0.1664909, 0.110445105, -0.10427...          1       4\n",
      "1580  [-0.00533887, 0.18308958, 0.11124927, -0.09639...          1       4\n",
      "1581  [-0.017801544, 0.18054993, 0.11661904, -0.0970...          1       4\n",
      "Game: 5, Number of Periods: 130\n",
      "                                   aggregated_embedding  EventType MatchID\n",
      "1747  [-0.00038975413, 0.15437879, 0.12025315, -0.07...          0       5\n",
      "1748  [0.0068489285, 0.15614574, 0.11954747, -0.0703...          0       5\n",
      "1749  [0.00029377686, 0.12992324, 0.113734506, -0.06...          1       5\n",
      "1750  [-0.018236635, 0.14483915, 0.096375115, -0.059...          0       5\n",
      "1751  [-0.020514153, 0.14651209, 0.10526787, -0.0583...          1       5\n",
      "Game: 7, Number of Periods: 130\n",
      "                                   aggregated_embedding  EventType MatchID\n",
      "1877  [0.013093434, 0.17267126, 0.1287539, -0.106067...          0       7\n",
      "1878  [0.014844747, 0.17122588, 0.13046792, -0.10654...          0       7\n",
      "1879  [0.042609755, 0.14664222, 0.16497582, -0.09160...          1       7\n",
      "1880  [0.024469215, 0.15026923, 0.14509334, -0.05194...          0       7\n",
      "1881  [0.01831436, 0.1707604, 0.15162861, -0.0426412...          0       7\n",
      "Game: 8, Number of Periods: 130\n",
      "                                   aggregated_embedding  EventType MatchID\n",
      "2007  [-0.09541339, 0.14481655, 0.012960993, -0.0114...          0       8\n",
      "2008  [-0.0937828, 0.13728885, 0.020146335, -0.01915...          0       8\n",
      "2009  [-0.12687334, 0.09719715, 0.025225066, -0.0064...          1       8\n",
      "2010  [-0.13221268, 0.081123576, 0.021790477, 0.0002...          1       8\n",
      "2011  [-0.122164614, 0.10765826, 0.019151986, -0.009...          0       8\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_and_split_embeddings(embeddings_path):\n",
    "    \"\"\"\n",
    "    Load precomputed embeddings from a pickle file and split them by game.\n",
    "\n",
    "    Args:\n",
    "        embeddings_path (str): Path to the pickle file containing embeddings.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where each key is a MatchID (game) and the value is a DataFrame\n",
    "              containing embeddings and labels for that game.\n",
    "    \"\"\"\n",
    "    # Load the pickle file into a DataFrame\n",
    "    print(\"Loading embeddings from pickle file...\")\n",
    "    merged_df = pd.read_pickle(embeddings_path)\n",
    "\n",
    "    # Extract the MatchID from the ID column\n",
    "    merged_df[\"MatchID\"] = merged_df[\"ID\"].apply(lambda x: x.split(\"_\")[0])  # Extract the MatchID\n",
    "    merged_df.drop(\"ID\", axis=1, inplace=True)\n",
    "    # merged_df.drop()\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Group by MatchID and create a dictionary of DataFrames\n",
    "    grouped_data = {match_id: group for match_id, group in merged_df.groupby(\"MatchID\")}\n",
    "\n",
    "    print(f\"Loaded and split data into {len(grouped_data)} games.\")\n",
    "    return grouped_data\n",
    "\n",
    "# Example Usage\n",
    "embeddings_path = \"aggregated_embeddings_with_labels.pkl\"\n",
    "game_data = load_and_split_embeddings(embeddings_path)\n",
    "\n",
    "# Example: Access data for a specific game\n",
    "for match_id, df in game_data.items():\n",
    "    print(f\"Game: {match_id}, Number of Periods: {len(df)}\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_data[\"0\"].columns, game_data[\"0\"].shape, game_data[\"0\"].head()\n",
    "# len(game_data[\"0\"][\"aggregated_embedding\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from aggregated_embeddings_with_labels.pkl...\n",
      "Loaded 2137 PeriodIDs.\n",
      "Loading embeddings from aggregated_embeddings_with_labels_test.pkl...\n",
      "Loaded 516 PeriodIDs.\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(file_path):\n",
    "    \"\"\"\n",
    "    Load embeddings and labels from the pickle file and extract MatchID from the PeriodID.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the pickle file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with MatchID, PeriodID, aggregated embeddings, and EventType.\n",
    "    \"\"\"\n",
    "    print(f\"Loading embeddings from {file_path}...\")\n",
    "    merged_df = pd.read_pickle(file_path)\n",
    "\n",
    "    # Extract MatchID from ID (assuming the ID format is 'MatchID_PeriodID')\n",
    "    if \"ID\" in merged_df.columns:\n",
    "        merged_df[\"MatchID\"] = merged_df[\"ID\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "    print(f\"Loaded {len(merged_df)} PeriodIDs.\")\n",
    "    return merged_df\n",
    "\n",
    "# Load training and testing data\n",
    "train_embeddings_file = \"aggregated_embeddings_with_labels.pkl\"\n",
    "test_embeddings_file = \"aggregated_embeddings_with_labels_test.pkl\"\n",
    "\n",
    "train_df = load_embeddings(train_embeddings_file)\n",
    "test_df = load_embeddings(test_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>aggregated_embedding</th>\n",
       "      <th>EventType</th>\n",
       "      <th>MatchID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_0</td>\n",
       "      <td>[0.038894046, 0.2056955, 0.1732219, -0.0507800...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_1</td>\n",
       "      <td>[0.049072143, 0.20504853, 0.16913632, -0.04101...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_10</td>\n",
       "      <td>[-0.010199165, 0.19084008, 0.142129, -0.027013...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_100</td>\n",
       "      <td>[0.1076907, 0.18990463, 0.13662358, -0.0059914...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_101</td>\n",
       "      <td>[0.110753596, 0.21662608, 0.14855266, -0.02414...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>8_95</td>\n",
       "      <td>[-0.12547426, 0.10333069, 0.012717849, 0.00890...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>8_96</td>\n",
       "      <td>[-0.13536794, 0.08814793, 0.013134736, -0.0146...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>8_97</td>\n",
       "      <td>[-0.13311367, 0.09247862, 0.016269028, 0.00072...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>8_98</td>\n",
       "      <td>[-0.12481897, 0.07730664, 0.014376387, 0.00727...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>8_99</td>\n",
       "      <td>[-0.13261126, 0.08881129, 0.017069561, -0.0076...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2137 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                               aggregated_embedding  EventType  \\\n",
       "0       0_0  [0.038894046, 0.2056955, 0.1732219, -0.0507800...          0   \n",
       "1       0_1  [0.049072143, 0.20504853, 0.16913632, -0.04101...          0   \n",
       "2      0_10  [-0.010199165, 0.19084008, 0.142129, -0.027013...          1   \n",
       "3     0_100  [0.1076907, 0.18990463, 0.13662358, -0.0059914...          1   \n",
       "4     0_101  [0.110753596, 0.21662608, 0.14855266, -0.02414...          1   \n",
       "...     ...                                                ...        ...   \n",
       "2132   8_95  [-0.12547426, 0.10333069, 0.012717849, 0.00890...          0   \n",
       "2133   8_96  [-0.13536794, 0.08814793, 0.013134736, -0.0146...          1   \n",
       "2134   8_97  [-0.13311367, 0.09247862, 0.016269028, 0.00072...          1   \n",
       "2135   8_98  [-0.12481897, 0.07730664, 0.014376387, 0.00727...          0   \n",
       "2136   8_99  [-0.13261126, 0.08881129, 0.017069561, -0.0076...          1   \n",
       "\n",
       "     MatchID  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "2132       8  \n",
       "2133       8  \n",
       "2134       8  \n",
       "2135       8  \n",
       "2136       8  \n",
       "\n",
       "[2137 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_game_based_data_loaders(df, batch_size, sequence_length=5):\n",
    "    game_loaders = {}\n",
    "    grouped = df.groupby(\"MatchID\")\n",
    "\n",
    "    for match_id, group in grouped:\n",
    "        # Convert embeddings to array\n",
    "        all_embeddings = np.vstack(group[\"aggregated_embedding\"].values).astype(np.float32)  # shape: (num_periods, embedding_dim)\n",
    "        all_labels = group[\"EventType\"].values  # shape: (num_periods,)\n",
    "\n",
    "        sequences = []\n",
    "        seq_labels = []\n",
    "\n",
    "        # Create sequences\n",
    "        for i in range(len(all_embeddings) - sequence_length + 1):\n",
    "            seq_emb = all_embeddings[i:i+sequence_length]  # shape: (seq_len, embedding_dim)\n",
    "            seq_label = all_labels[i+sequence_length-1]    # Label of the last element in the sequence\n",
    "            sequences.append(seq_emb)\n",
    "            seq_labels.append(seq_label)\n",
    "\n",
    "        sequences = np.array(sequences)        # shape: (num_samples, seq_len, embedding_dim)\n",
    "        seq_labels = np.array(seq_labels)      # shape: (num_samples,)\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(sequences, dtype=torch.float32), \n",
    "            torch.tensor(seq_labels, dtype=torch.float32)\n",
    "        )\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        game_loaders[match_id] = loader\n",
    "\n",
    "    return game_loaders\n",
    "\n",
    "# Split train data for cross-validation\n",
    "train_df_cv, val_df_cv = train_test_split(\n",
    "    train_df, test_size=0.2, random_state=42, stratify=train_df[\"MatchID\"]\n",
    ")\n",
    "\n",
    "# Create game-based data loaders\n",
    "train_game_loaders = create_game_based_data_loaders(train_df_cv, batch_size=32, sequence_length=3)\n",
    "val_game_loaders = create_game_based_data_loaders(val_df_cv, batch_size=32, sequence_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=84, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronologicalLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=84, hidden_dim=126, num_layers=2, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # Hidden size of the LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=0.2,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # hidden_dim doubled due to bidirectionality\n",
    "        self.intermediate_dim = hidden_dim * 2\n",
    "\n",
    "        # Additional feed-forward layer\n",
    "        self.fc1 = nn.Linear(self.intermediate_dim, self.intermediate_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.classifier = nn.Linear(self.intermediate_dim, 1)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # embeddings: (batch_size, seq_len, input_dim)\n",
    "        _, (hidden, _) = self.lstm(embeddings)\n",
    "        # hidden: (num_layers*2, batch_size, hidden_dim)\n",
    "        # Extract the last layer's forward and backward hidden states:\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden_state = torch.cat((hidden[-2], hidden[-1]), dim=-1)  # Concatenate forward and backward\n",
    "        else:\n",
    "            hidden_state = hidden[-1]\n",
    "\n",
    "        x = self.fc1(hidden_state)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        logits = self.classifier(x)  # (batch_size, 1)\n",
    "        return torch.sigmoid(logits).squeeze(-1)  # (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=84, num_heads=2, hidden_dim=84, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)  # Match input_dim with embedding\n",
    "        self.projection = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = PositionalEncoding(d_model=hidden_dim)\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=0.2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # embeddings: (batch_size, seq_len, input_dim)\n",
    "        x = self.projection(embeddings)    # (batch_size, seq_len, hidden_dim)\n",
    "        x = self.pos_encoder(x)            # Add positional encoding\n",
    "        x = self.transformer(x)            # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        pooled_output = x.mean(dim=1)      # (batch_size, hidden_dim)\n",
    "        x = self.fc1(pooled_output)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        logits = self.classifier(x)        # (batch_size, 1)\n",
    "        return torch.sigmoid(logits).squeeze(-1)  # (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2024/elio.samaha/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_game_based_model(model, game_loaders, val_loader, epochs, lr, device):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Calculate total number of training steps\n",
    "    total_batches = sum(len(loader) for loader in game_loaders.values())\n",
    "    total_steps = epochs * total_batches\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=warmup_steps, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        total_train_batches = 0\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}...\")\n",
    "        for match_id, game_loader in game_loaders.items():\n",
    "            for embeddings, labels in game_loader:\n",
    "                embeddings, labels = embeddings.to(device), labels.float().to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(embeddings)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                preds = (outputs.detach().cpu().numpy() > 0.5).astype(int)\n",
    "                train_accuracy += accuracy_score(labels.cpu().numpy(), preds)\n",
    "                total_train_batches += 1\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        total_val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for match_id, val_game_loader in val_loader.items():\n",
    "                for embeddings, labels in val_game_loader:\n",
    "                    embeddings, labels = embeddings.to(device), labels.float().to(device)\n",
    "                    outputs = model(embeddings)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    preds = (outputs.cpu().numpy() > 0.5).astype(int)\n",
    "                    val_accuracy += accuracy_score(labels.cpu().numpy(), preds)\n",
    "                    total_val_batches += 1\n",
    "\n",
    "        avg_train_loss = train_loss / total_train_batches\n",
    "        avg_train_acc = train_accuracy / total_train_batches\n",
    "        avg_val_loss = val_loss / total_val_batches\n",
    "        avg_val_acc = val_accuracy / total_val_batches\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss {avg_train_loss:.4f}, Train Acc {avg_train_acc:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Acc {avg_val_acc:.4f}\")\n",
    "\n",
    "        # Track best validation accuracy\n",
    "        if avg_val_acc > best_val_acc:\n",
    "            best_val_acc = avg_val_acc\n",
    "            # Optionally save the model if you wish\n",
    "            # torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_comparison(models, val_game_loaders, device):\n",
    "    \"\"\"\n",
    "    Evaluate and compare multiple models on the validation set.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Dictionary where keys are model names and values are trained models.\n",
    "        val_game_loaders (dict): Validation loaders grouped by game.\n",
    "        device: Computation device (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing metrics for each model.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for match_id, val_loader in val_game_loaders.items():\n",
    "                for embeddings, labels in val_loader:\n",
    "                    embeddings, labels = embeddings.to(device), labels.float().to(device)  # Ensure labels are float\n",
    "                    outputs = model(embeddings).squeeze(-1)  # Ensure shape matches (batch_size,)\n",
    "                    preds = (outputs > 0.5).int()\n",
    "\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Chronological LSTM...\n",
      "Epoch 1/80...\n",
      "Epoch 1: Train Loss 0.6940, Train Acc 0.4771\n",
      "Validation Loss: 0.6952, Validation Acc 0.4216\n",
      "Epoch 2/80...\n",
      "Epoch 2: Train Loss 0.6935, Train Acc 0.4858\n",
      "Validation Loss: 0.6934, Validation Acc 0.4265\n",
      "Epoch 3/80...\n",
      "Epoch 3: Train Loss 0.6926, Train Acc 0.5153\n",
      "Validation Loss: 0.6910, Validation Acc 0.5509\n",
      "Epoch 4/80...\n",
      "Epoch 4: Train Loss 0.6923, Train Acc 0.5235\n",
      "Validation Loss: 0.6888, Validation Acc 0.5784\n",
      "Epoch 5/80...\n",
      "Epoch 5: Train Loss 0.6914, Train Acc 0.5328\n",
      "Validation Loss: 0.6891, Validation Acc 0.5509\n",
      "Epoch 6/80...\n",
      "Epoch 6: Train Loss 0.6882, Train Acc 0.5489\n",
      "Validation Loss: 0.6824, Validation Acc 0.5784\n",
      "Epoch 7/80...\n",
      "Epoch 7: Train Loss 0.6911, Train Acc 0.5363\n",
      "Validation Loss: 0.6876, Validation Acc 0.5509\n",
      "Epoch 8/80...\n",
      "Epoch 8: Train Loss 0.6905, Train Acc 0.5276\n",
      "Validation Loss: 0.6877, Validation Acc 0.6085\n",
      "Epoch 9/80...\n",
      "Epoch 9: Train Loss 0.6905, Train Acc 0.5379\n",
      "Validation Loss: 0.6910, Validation Acc 0.5376\n",
      "Epoch 10/80...\n",
      "Epoch 10: Train Loss 0.6800, Train Acc 0.5496\n",
      "Validation Loss: 0.6751, Validation Acc 0.6036\n",
      "Epoch 11/80...\n",
      "Epoch 11: Train Loss 0.6785, Train Acc 0.5665\n",
      "Validation Loss: 0.6755, Validation Acc 0.5692\n",
      "Epoch 12/80...\n",
      "Epoch 12: Train Loss 0.6715, Train Acc 0.5769\n",
      "Validation Loss: 0.6821, Validation Acc 0.5439\n",
      "Epoch 13/80...\n",
      "Epoch 13: Train Loss 0.6621, Train Acc 0.5966\n",
      "Validation Loss: 0.6861, Validation Acc 0.5153\n",
      "Epoch 14/80...\n",
      "Epoch 14: Train Loss 0.6709, Train Acc 0.5398\n",
      "Validation Loss: 0.6858, Validation Acc 0.5055\n",
      "Epoch 15/80...\n",
      "Epoch 15: Train Loss 0.6942, Train Acc 0.5116\n",
      "Validation Loss: 0.6919, Validation Acc 0.4786\n",
      "Epoch 16/80...\n",
      "Epoch 16: Train Loss 0.6745, Train Acc 0.5668\n",
      "Validation Loss: 0.6641, Validation Acc 0.6134\n",
      "Epoch 17/80...\n",
      "Epoch 17: Train Loss 0.6618, Train Acc 0.5986\n",
      "Validation Loss: 0.6756, Validation Acc 0.5464\n",
      "Epoch 18/80...\n",
      "Epoch 18: Train Loss 0.6522, Train Acc 0.5882\n",
      "Validation Loss: 0.6743, Validation Acc 0.5335\n",
      "Epoch 19/80...\n",
      "Epoch 19: Train Loss 0.6435, Train Acc 0.6114\n",
      "Validation Loss: 0.6745, Validation Acc 0.5384\n",
      "Epoch 20/80...\n",
      "Epoch 20: Train Loss 0.6535, Train Acc 0.5944\n",
      "Validation Loss: 0.6779, Validation Acc 0.5357\n",
      "Epoch 21/80...\n",
      "Epoch 21: Train Loss 0.6865, Train Acc 0.5145\n",
      "Validation Loss: 0.6765, Validation Acc 0.6093\n",
      "Epoch 22/80...\n",
      "Epoch 22: Train Loss 0.6445, Train Acc 0.6230\n",
      "Validation Loss: 0.6655, Validation Acc 0.5370\n",
      "Epoch 23/80...\n",
      "Epoch 23: Train Loss 0.6328, Train Acc 0.6205\n",
      "Validation Loss: 0.6666, Validation Acc 0.5400\n",
      "Epoch 24/80...\n",
      "Epoch 24: Train Loss 0.6273, Train Acc 0.6320\n",
      "Validation Loss: 0.6492, Validation Acc 0.5856\n",
      "Epoch 25/80...\n",
      "Epoch 25: Train Loss 0.6402, Train Acc 0.6313\n",
      "Validation Loss: 0.6524, Validation Acc 0.5831\n",
      "Epoch 26/80...\n",
      "Epoch 26: Train Loss 0.6410, Train Acc 0.6061\n",
      "Validation Loss: 0.6237, Validation Acc 0.6403\n",
      "Epoch 27/80...\n",
      "Epoch 27: Train Loss 0.6100, Train Acc 0.6478\n",
      "Validation Loss: 0.6218, Validation Acc 0.6168\n",
      "Epoch 28/80...\n",
      "Epoch 28: Train Loss 0.6095, Train Acc 0.6465\n",
      "Validation Loss: 0.6022, Validation Acc 0.6601\n",
      "Epoch 29/80...\n",
      "Epoch 29: Train Loss 0.5932, Train Acc 0.6817\n",
      "Validation Loss: 0.6115, Validation Acc 0.6736\n",
      "Epoch 30/80...\n",
      "Epoch 30: Train Loss 0.5750, Train Acc 0.6914\n",
      "Validation Loss: 0.6141, Validation Acc 0.6436\n",
      "Epoch 31/80...\n",
      "Epoch 31: Train Loss 0.5975, Train Acc 0.6853\n",
      "Validation Loss: 0.5752, Validation Acc 0.7067\n",
      "Epoch 32/80...\n",
      "Epoch 32: Train Loss 0.5876, Train Acc 0.6863\n",
      "Validation Loss: 0.5816, Validation Acc 0.6944\n",
      "Epoch 33/80...\n",
      "Epoch 33: Train Loss 0.5779, Train Acc 0.6882\n",
      "Validation Loss: 0.5851, Validation Acc 0.6763\n",
      "Epoch 34/80...\n",
      "Epoch 34: Train Loss 0.5743, Train Acc 0.7055\n",
      "Validation Loss: 0.5855, Validation Acc 0.6614\n",
      "Epoch 35/80...\n",
      "Epoch 35: Train Loss 0.5551, Train Acc 0.7213\n",
      "Validation Loss: 0.5746, Validation Acc 0.6902\n",
      "Epoch 36/80...\n",
      "Epoch 36: Train Loss 0.5741, Train Acc 0.7056\n",
      "Validation Loss: 0.5809, Validation Acc 0.6718\n",
      "Epoch 37/80...\n",
      "Epoch 37: Train Loss 0.5630, Train Acc 0.7048\n",
      "Validation Loss: 0.5529, Validation Acc 0.7143\n",
      "Epoch 38/80...\n",
      "Epoch 38: Train Loss 0.5470, Train Acc 0.7141\n",
      "Validation Loss: 0.5764, Validation Acc 0.6754\n",
      "Epoch 39/80...\n",
      "Epoch 39: Train Loss 0.5423, Train Acc 0.7094\n",
      "Validation Loss: 0.5710, Validation Acc 0.6883\n",
      "Epoch 40/80...\n",
      "Epoch 40: Train Loss 0.5415, Train Acc 0.7185\n",
      "Validation Loss: 0.5682, Validation Acc 0.6996\n",
      "Epoch 41/80...\n",
      "Epoch 41: Train Loss 0.5422, Train Acc 0.7275\n",
      "Validation Loss: 0.5811, Validation Acc 0.6771\n",
      "Epoch 42/80...\n",
      "Epoch 42: Train Loss 0.5363, Train Acc 0.7390\n",
      "Validation Loss: 0.5549, Validation Acc 0.7333\n",
      "Epoch 43/80...\n",
      "Epoch 43: Train Loss 0.5426, Train Acc 0.7207\n",
      "Validation Loss: 0.5876, Validation Acc 0.6808\n",
      "Epoch 44/80...\n",
      "Epoch 44: Train Loss 0.5413, Train Acc 0.7318\n",
      "Validation Loss: 0.6081, Validation Acc 0.6534\n",
      "Epoch 45/80...\n",
      "Epoch 45: Train Loss 0.5430, Train Acc 0.7197\n",
      "Validation Loss: 0.5528, Validation Acc 0.7141\n",
      "Epoch 46/80...\n",
      "Epoch 46: Train Loss 0.5620, Train Acc 0.7004\n",
      "Validation Loss: 0.5754, Validation Acc 0.6938\n",
      "Epoch 47/80...\n",
      "Epoch 47: Train Loss 0.5437, Train Acc 0.7275\n",
      "Validation Loss: 0.6475, Validation Acc 0.6240\n",
      "Epoch 48/80...\n",
      "Epoch 48: Train Loss 0.5593, Train Acc 0.7009\n",
      "Validation Loss: 0.6357, Validation Acc 0.6375\n",
      "Epoch 49/80...\n",
      "Epoch 49: Train Loss 0.5566, Train Acc 0.7119\n",
      "Validation Loss: 0.6821, Validation Acc 0.6044\n",
      "Epoch 50/80...\n",
      "Epoch 50: Train Loss 0.5424, Train Acc 0.7260\n",
      "Validation Loss: 0.5617, Validation Acc 0.7012\n",
      "Epoch 51/80...\n",
      "Epoch 51: Train Loss 0.5307, Train Acc 0.7219\n",
      "Validation Loss: 0.6583, Validation Acc 0.6469\n",
      "Epoch 52/80...\n",
      "Epoch 52: Train Loss 0.5447, Train Acc 0.7078\n",
      "Validation Loss: 0.6104, Validation Acc 0.6652\n",
      "Epoch 53/80...\n",
      "Epoch 53: Train Loss 0.5390, Train Acc 0.7250\n",
      "Validation Loss: 0.5932, Validation Acc 0.6714\n",
      "Epoch 54/80...\n",
      "Epoch 54: Train Loss 0.5309, Train Acc 0.7259\n",
      "Validation Loss: 0.5763, Validation Acc 0.6946\n",
      "Epoch 55/80...\n",
      "Epoch 55: Train Loss 0.5179, Train Acc 0.7280\n",
      "Validation Loss: 0.5686, Validation Acc 0.7012\n",
      "Epoch 56/80...\n",
      "Epoch 56: Train Loss 0.5294, Train Acc 0.7270\n",
      "Validation Loss: 0.6369, Validation Acc 0.6377\n",
      "Epoch 57/80...\n",
      "Epoch 57: Train Loss 0.5357, Train Acc 0.7237\n",
      "Validation Loss: 0.6230, Validation Acc 0.6530\n",
      "Epoch 58/80...\n",
      "Epoch 58: Train Loss 0.5316, Train Acc 0.7323\n",
      "Validation Loss: 0.5636, Validation Acc 0.7134\n",
      "Epoch 59/80...\n",
      "Epoch 59: Train Loss 0.5290, Train Acc 0.7299\n",
      "Validation Loss: 0.5463, Validation Acc 0.7110\n",
      "Epoch 60/80...\n",
      "Epoch 60: Train Loss 0.5234, Train Acc 0.7245\n",
      "Validation Loss: 0.5947, Validation Acc 0.6944\n",
      "Epoch 61/80...\n",
      "Epoch 61: Train Loss 0.5315, Train Acc 0.7277\n",
      "Validation Loss: 0.6086, Validation Acc 0.6597\n",
      "Epoch 62/80...\n",
      "Epoch 62: Train Loss 0.5119, Train Acc 0.7393\n",
      "Validation Loss: 0.5568, Validation Acc 0.7220\n",
      "Epoch 63/80...\n",
      "Epoch 63: Train Loss 0.5350, Train Acc 0.7313\n",
      "Validation Loss: 0.5424, Validation Acc 0.7167\n",
      "Epoch 64/80...\n",
      "Epoch 64: Train Loss 0.5352, Train Acc 0.7198\n",
      "Validation Loss: 0.5618, Validation Acc 0.7122\n",
      "Epoch 65/80...\n",
      "Epoch 65: Train Loss 0.5405, Train Acc 0.7111\n",
      "Validation Loss: 0.5810, Validation Acc 0.6865\n",
      "Epoch 66/80...\n",
      "Epoch 66: Train Loss 0.5136, Train Acc 0.7313\n",
      "Validation Loss: 0.6089, Validation Acc 0.6622\n",
      "Epoch 67/80...\n",
      "Epoch 67: Train Loss 0.5160, Train Acc 0.7454\n",
      "Validation Loss: 0.6400, Validation Acc 0.6530\n",
      "Epoch 68/80...\n",
      "Epoch 68: Train Loss 0.5152, Train Acc 0.7485\n",
      "Validation Loss: 0.5816, Validation Acc 0.6969\n",
      "Epoch 69/80...\n",
      "Epoch 69: Train Loss 0.5300, Train Acc 0.7381\n",
      "Validation Loss: 0.5769, Validation Acc 0.7081\n",
      "Epoch 70/80...\n",
      "Epoch 70: Train Loss 0.5020, Train Acc 0.7486\n",
      "Validation Loss: 0.5975, Validation Acc 0.6830\n",
      "Epoch 71/80...\n",
      "Epoch 71: Train Loss 0.5321, Train Acc 0.7188\n",
      "Validation Loss: 0.5883, Validation Acc 0.6848\n",
      "Epoch 72/80...\n",
      "Epoch 72: Train Loss 0.5197, Train Acc 0.7327\n",
      "Validation Loss: 0.5982, Validation Acc 0.6840\n",
      "Epoch 73/80...\n",
      "Epoch 73: Train Loss 0.5244, Train Acc 0.7129\n",
      "Validation Loss: 0.5551, Validation Acc 0.7294\n",
      "Epoch 74/80...\n",
      "Epoch 74: Train Loss 0.5203, Train Acc 0.7293\n",
      "Validation Loss: 0.5456, Validation Acc 0.7283\n",
      "Epoch 75/80...\n",
      "Epoch 75: Train Loss 0.5293, Train Acc 0.7257\n",
      "Validation Loss: 0.5502, Validation Acc 0.7281\n",
      "Epoch 76/80...\n",
      "Epoch 76: Train Loss 0.5132, Train Acc 0.7390\n",
      "Validation Loss: 0.5397, Validation Acc 0.7283\n",
      "Epoch 77/80...\n",
      "Epoch 77: Train Loss 0.5222, Train Acc 0.7241\n",
      "Validation Loss: 0.5765, Validation Acc 0.6983\n",
      "Epoch 78/80...\n",
      "Epoch 78: Train Loss 0.5257, Train Acc 0.7316\n",
      "Validation Loss: 0.5445, Validation Acc 0.7192\n",
      "Epoch 79/80...\n",
      "Epoch 79: Train Loss 0.5195, Train Acc 0.7341\n",
      "Validation Loss: 0.5772, Validation Acc 0.6891\n",
      "Epoch 80/80...\n",
      "Epoch 80: Train Loss 0.5185, Train Acc 0.7319\n",
      "Validation Loss: 0.5716, Validation Acc 0.6916\n",
      "Training Transformer Encoder...\n",
      "Epoch 1/80...\n",
      "Epoch 1: Train Loss 0.6923, Train Acc 0.5282\n",
      "Validation Loss: 0.6808, Validation Acc 0.6060\n",
      "Epoch 2/80...\n",
      "Epoch 2: Train Loss 0.6960, Train Acc 0.4853\n",
      "Validation Loss: 0.6938, Validation Acc 0.4706\n",
      "Epoch 3/80...\n",
      "Epoch 3: Train Loss 0.6947, Train Acc 0.5062\n",
      "Validation Loss: 0.6901, Validation Acc 0.5760\n",
      "Epoch 4/80...\n",
      "Epoch 4: Train Loss 0.6970, Train Acc 0.4867\n",
      "Validation Loss: 0.6921, Validation Acc 0.5758\n",
      "Epoch 5/80...\n",
      "Epoch 5: Train Loss 0.6961, Train Acc 0.5140\n",
      "Validation Loss: 0.6869, Validation Acc 0.5509\n",
      "Epoch 6/80...\n",
      "Epoch 6: Train Loss 0.6996, Train Acc 0.4954\n",
      "Validation Loss: 0.6885, Validation Acc 0.5784\n",
      "Epoch 7/80...\n",
      "Epoch 7: Train Loss 0.6939, Train Acc 0.5216\n",
      "Validation Loss: 0.6798, Validation Acc 0.5784\n",
      "Epoch 8/80...\n",
      "Epoch 8: Train Loss 0.7011, Train Acc 0.4996\n",
      "Validation Loss: 0.6895, Validation Acc 0.5656\n",
      "Epoch 9/80...\n",
      "Epoch 9: Train Loss 0.6933, Train Acc 0.5291\n",
      "Validation Loss: 0.6857, Validation Acc 0.5784\n",
      "Epoch 10/80...\n",
      "Epoch 10: Train Loss 0.6961, Train Acc 0.4966\n",
      "Validation Loss: 0.6974, Validation Acc 0.4436\n",
      "Epoch 11/80...\n",
      "Epoch 11: Train Loss 0.6911, Train Acc 0.5433\n",
      "Validation Loss: 0.6844, Validation Acc 0.6060\n",
      "Epoch 12/80...\n",
      "Epoch 12: Train Loss 0.6910, Train Acc 0.5443\n",
      "Validation Loss: 0.6823, Validation Acc 0.5784\n",
      "Epoch 13/80...\n",
      "Epoch 13: Train Loss 0.6899, Train Acc 0.5436\n",
      "Validation Loss: 0.6867, Validation Acc 0.6060\n",
      "Epoch 14/80...\n",
      "Epoch 14: Train Loss 0.6936, Train Acc 0.5366\n",
      "Validation Loss: 0.6872, Validation Acc 0.5913\n",
      "Epoch 15/80...\n",
      "Epoch 15: Train Loss 0.6920, Train Acc 0.5267\n",
      "Validation Loss: 0.7024, Validation Acc 0.4381\n",
      "Epoch 16/80...\n",
      "Epoch 16: Train Loss 0.6825, Train Acc 0.5272\n",
      "Validation Loss: 0.6759, Validation Acc 0.5509\n",
      "Epoch 17/80...\n",
      "Epoch 17: Train Loss 0.6832, Train Acc 0.5422\n",
      "Validation Loss: 0.6791, Validation Acc 0.5509\n",
      "Epoch 18/80...\n",
      "Epoch 18: Train Loss 0.6743, Train Acc 0.5580\n",
      "Validation Loss: 0.6745, Validation Acc 0.5711\n",
      "Epoch 19/80...\n",
      "Epoch 19: Train Loss 0.6622, Train Acc 0.5985\n",
      "Validation Loss: 0.7552, Validation Acc 0.5065\n",
      "Epoch 20/80...\n",
      "Epoch 20: Train Loss 0.6355, Train Acc 0.6128\n",
      "Validation Loss: 0.8239, Validation Acc 0.4861\n",
      "Epoch 21/80...\n",
      "Epoch 21: Train Loss 0.6595, Train Acc 0.5900\n",
      "Validation Loss: 0.6889, Validation Acc 0.5139\n",
      "Epoch 22/80...\n",
      "Epoch 22: Train Loss 0.6385, Train Acc 0.6339\n",
      "Validation Loss: 0.6331, Validation Acc 0.5995\n",
      "Epoch 23/80...\n",
      "Epoch 23: Train Loss 0.6174, Train Acc 0.6407\n",
      "Validation Loss: 0.6319, Validation Acc 0.5958\n",
      "Epoch 24/80...\n",
      "Epoch 24: Train Loss 0.6016, Train Acc 0.6680\n",
      "Validation Loss: 0.6564, Validation Acc 0.6007\n",
      "Epoch 25/80...\n",
      "Epoch 25: Train Loss 0.6010, Train Acc 0.6901\n",
      "Validation Loss: 0.6114, Validation Acc 0.6603\n",
      "Epoch 26/80...\n",
      "Epoch 26: Train Loss 0.5834, Train Acc 0.7017\n",
      "Validation Loss: 0.5992, Validation Acc 0.6648\n",
      "Epoch 27/80...\n",
      "Epoch 27: Train Loss 0.5963, Train Acc 0.6820\n",
      "Validation Loss: 0.6183, Validation Acc 0.6328\n",
      "Epoch 28/80...\n",
      "Epoch 28: Train Loss 0.5766, Train Acc 0.7040\n",
      "Validation Loss: 0.6330, Validation Acc 0.6495\n",
      "Epoch 29/80...\n",
      "Epoch 29: Train Loss 0.5658, Train Acc 0.7136\n",
      "Validation Loss: 0.7260, Validation Acc 0.6330\n",
      "Epoch 30/80...\n",
      "Epoch 30: Train Loss 0.5582, Train Acc 0.7203\n",
      "Validation Loss: 0.6233, Validation Acc 0.6446\n",
      "Epoch 31/80...\n",
      "Epoch 31: Train Loss 0.5659, Train Acc 0.7164\n",
      "Validation Loss: 0.6447, Validation Acc 0.6522\n",
      "Epoch 32/80...\n",
      "Epoch 32: Train Loss 0.5696, Train Acc 0.7040\n",
      "Validation Loss: 0.6176, Validation Acc 0.6385\n",
      "Epoch 33/80...\n",
      "Epoch 33: Train Loss 0.5646, Train Acc 0.7154\n",
      "Validation Loss: 0.5998, Validation Acc 0.6908\n",
      "Epoch 34/80...\n",
      "Epoch 34: Train Loss 0.5726, Train Acc 0.7013\n",
      "Validation Loss: 0.6023, Validation Acc 0.6716\n",
      "Epoch 35/80...\n",
      "Epoch 35: Train Loss 0.5687, Train Acc 0.7096\n",
      "Validation Loss: 0.5871, Validation Acc 0.6642\n",
      "Epoch 36/80...\n",
      "Epoch 36: Train Loss 0.5480, Train Acc 0.7366\n",
      "Validation Loss: 0.6116, Validation Acc 0.6716\n",
      "Epoch 37/80...\n",
      "Epoch 37: Train Loss 0.5430, Train Acc 0.7318\n",
      "Validation Loss: 0.6441, Validation Acc 0.6724\n",
      "Epoch 38/80...\n",
      "Epoch 38: Train Loss 0.5312, Train Acc 0.7299\n",
      "Validation Loss: 0.6401, Validation Acc 0.6266\n",
      "Epoch 39/80...\n",
      "Epoch 39: Train Loss 0.5240, Train Acc 0.7377\n",
      "Validation Loss: 0.5867, Validation Acc 0.7014\n",
      "Epoch 40/80...\n",
      "Epoch 40: Train Loss 0.5346, Train Acc 0.7249\n",
      "Validation Loss: 0.5800, Validation Acc 0.6667\n",
      "Epoch 41/80...\n",
      "Epoch 41: Train Loss 0.5324, Train Acc 0.7297\n",
      "Validation Loss: 0.6194, Validation Acc 0.6742\n",
      "Epoch 42/80...\n",
      "Epoch 42: Train Loss 0.5378, Train Acc 0.7268\n",
      "Validation Loss: 0.6021, Validation Acc 0.6902\n",
      "Epoch 43/80...\n",
      "Epoch 43: Train Loss 0.5215, Train Acc 0.7463\n",
      "Validation Loss: 0.6331, Validation Acc 0.6244\n",
      "Epoch 44/80...\n",
      "Epoch 44: Train Loss 0.5265, Train Acc 0.7402\n",
      "Validation Loss: 0.6156, Validation Acc 0.7061\n",
      "Epoch 45/80...\n",
      "Epoch 45: Train Loss 0.5447, Train Acc 0.7322\n",
      "Validation Loss: 0.5787, Validation Acc 0.7196\n",
      "Epoch 46/80...\n",
      "Epoch 46: Train Loss 0.5191, Train Acc 0.7470\n",
      "Validation Loss: 0.6492, Validation Acc 0.6920\n",
      "Epoch 47/80...\n",
      "Epoch 47: Train Loss 0.5264, Train Acc 0.7342\n",
      "Validation Loss: 0.6558, Validation Acc 0.6736\n",
      "Epoch 48/80...\n",
      "Epoch 48: Train Loss 0.5092, Train Acc 0.7511\n",
      "Validation Loss: 0.5721, Validation Acc 0.7196\n",
      "Epoch 49/80...\n",
      "Epoch 49: Train Loss 0.5076, Train Acc 0.7418\n",
      "Validation Loss: 0.5657, Validation Acc 0.7271\n",
      "Epoch 50/80...\n",
      "Epoch 50: Train Loss 0.4939, Train Acc 0.7588\n",
      "Validation Loss: 0.5905, Validation Acc 0.6963\n",
      "Epoch 51/80...\n",
      "Epoch 51: Train Loss 0.5269, Train Acc 0.7407\n",
      "Validation Loss: 0.6808, Validation Acc 0.6560\n",
      "Epoch 52/80...\n",
      "Epoch 52: Train Loss 0.4905, Train Acc 0.7653\n",
      "Validation Loss: 0.5658, Validation Acc 0.7361\n",
      "Epoch 53/80...\n",
      "Epoch 53: Train Loss 0.5150, Train Acc 0.7430\n",
      "Validation Loss: 0.5853, Validation Acc 0.7269\n",
      "Epoch 54/80...\n",
      "Epoch 54: Train Loss 0.5186, Train Acc 0.7353\n",
      "Validation Loss: 0.5551, Validation Acc 0.7192\n",
      "Epoch 55/80...\n",
      "Epoch 55: Train Loss 0.5133, Train Acc 0.7471\n",
      "Validation Loss: 0.5756, Validation Acc 0.6971\n",
      "Epoch 56/80...\n",
      "Epoch 56: Train Loss 0.4964, Train Acc 0.7467\n",
      "Validation Loss: 0.5678, Validation Acc 0.7012\n",
      "Epoch 57/80...\n",
      "Epoch 57: Train Loss 0.4850, Train Acc 0.7660\n",
      "Validation Loss: 0.5659, Validation Acc 0.6859\n",
      "Epoch 58/80...\n",
      "Epoch 58: Train Loss 0.5150, Train Acc 0.7600\n",
      "Validation Loss: 0.5526, Validation Acc 0.7296\n",
      "Epoch 59/80...\n",
      "Epoch 59: Train Loss 0.5008, Train Acc 0.7551\n",
      "Validation Loss: 0.5561, Validation Acc 0.7136\n",
      "Epoch 60/80...\n",
      "Epoch 60: Train Loss 0.5020, Train Acc 0.7461\n",
      "Validation Loss: 0.5834, Validation Acc 0.6926\n",
      "Epoch 61/80...\n",
      "Epoch 61: Train Loss 0.4980, Train Acc 0.7574\n",
      "Validation Loss: 0.5949, Validation Acc 0.6881\n",
      "Epoch 62/80...\n",
      "Epoch 62: Train Loss 0.4943, Train Acc 0.7525\n",
      "Validation Loss: 0.5650, Validation Acc 0.6930\n",
      "Epoch 63/80...\n",
      "Epoch 63: Train Loss 0.5025, Train Acc 0.7533\n",
      "Validation Loss: 0.6099, Validation Acc 0.6969\n",
      "Epoch 64/80...\n",
      "Epoch 64: Train Loss 0.4799, Train Acc 0.7654\n",
      "Validation Loss: 0.5690, Validation Acc 0.7002\n",
      "Epoch 65/80...\n",
      "Epoch 65: Train Loss 0.4962, Train Acc 0.7570\n",
      "Validation Loss: 0.6004, Validation Acc 0.6828\n",
      "Epoch 66/80...\n",
      "Epoch 66: Train Loss 0.4843, Train Acc 0.7676\n",
      "Validation Loss: 0.5493, Validation Acc 0.7112\n",
      "Epoch 67/80...\n",
      "Epoch 67: Train Loss 0.5101, Train Acc 0.7449\n",
      "Validation Loss: 0.5824, Validation Acc 0.6914\n",
      "Epoch 68/80...\n",
      "Epoch 68: Train Loss 0.5043, Train Acc 0.7451\n",
      "Validation Loss: 0.5463, Validation Acc 0.7234\n",
      "Epoch 69/80...\n",
      "Epoch 69: Train Loss 0.4911, Train Acc 0.7595\n",
      "Validation Loss: 0.5687, Validation Acc 0.6804\n",
      "Epoch 70/80...\n",
      "Epoch 70: Train Loss 0.4881, Train Acc 0.7603\n",
      "Validation Loss: 0.5632, Validation Acc 0.6993\n",
      "Epoch 71/80...\n",
      "Epoch 71: Train Loss 0.4970, Train Acc 0.7575\n",
      "Validation Loss: 0.5586, Validation Acc 0.7026\n",
      "Epoch 72/80...\n",
      "Epoch 72: Train Loss 0.5031, Train Acc 0.7407\n",
      "Validation Loss: 0.5970, Validation Acc 0.6763\n",
      "Epoch 73/80...\n",
      "Epoch 73: Train Loss 0.4877, Train Acc 0.7652\n",
      "Validation Loss: 0.5428, Validation Acc 0.7155\n",
      "Epoch 74/80...\n",
      "Epoch 74: Train Loss 0.4876, Train Acc 0.7544\n",
      "Validation Loss: 0.5875, Validation Acc 0.6879\n",
      "Epoch 75/80...\n",
      "Epoch 75: Train Loss 0.4899, Train Acc 0.7642\n",
      "Validation Loss: 0.5486, Validation Acc 0.7022\n",
      "Epoch 76/80...\n",
      "Epoch 76: Train Loss 0.4974, Train Acc 0.7523\n",
      "Validation Loss: 0.5640, Validation Acc 0.6928\n",
      "Epoch 77/80...\n",
      "Epoch 77: Train Loss 0.4714, Train Acc 0.7770\n",
      "Validation Loss: 0.6214, Validation Acc 0.6887\n",
      "Epoch 78/80...\n",
      "Epoch 78: Train Loss 0.4875, Train Acc 0.7486\n",
      "Validation Loss: 0.5552, Validation Acc 0.6998\n",
      "Epoch 79/80...\n",
      "Epoch 79: Train Loss 0.4852, Train Acc 0.7641\n",
      "Validation Loss: 0.5806, Validation Acc 0.6771\n",
      "Epoch 80/80...\n",
      "Epoch 80: Train Loss 0.4817, Train Acc 0.7636\n",
      "Validation Loss: 0.5502, Validation Acc 0.7022\n",
      "Evaluating Chronological LSTM...\n",
      "Evaluating Transformer Encoder...\n",
      "                 Model  Accuracy  Precision    Recall  F1-Score\n",
      "0   Chronological LSTM  0.704545   0.761682  0.711790  0.735892\n",
      "1  Transformer Encoder  0.686869   0.780749  0.637555  0.701923\n"
     ]
    }
   ],
   "source": [
    "# Train Models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lstm_model = ChronologicalLSTM(input_dim=768, hidden_dim=128)\n",
    "transformer_model = TransformerEncoder(input_dim=768, hidden_dim=128, num_heads=4, num_layers=2)\n",
    "\n",
    "print(\"Training Chronological LSTM...\")\n",
    "trained_lstm_model = train_game_based_model(lstm_model, train_game_loaders, val_game_loaders, epochs=80, lr=1e-4, device=device)\n",
    "\n",
    "print(\"Training Transformer Encoder...\")\n",
    "trained_transformer_model = train_game_based_model(transformer_model, train_game_loaders, val_game_loaders, epochs=80, lr=1e-4, device=device)\n",
    "\n",
    "# Compare Models\n",
    "comparison_df = evaluate_model_comparison(\n",
    "    {\"Chronological LSTM\": trained_lstm_model, \"Transformer Encoder\": trained_transformer_model},\n",
    "    val_game_loaders, device\n",
    ")\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:51:28,472] A new study created in memory with name: no-name-7eddd007-b1ea-4b9a-9661-a23ca7afb360\n",
      "[I 2024-12-05 21:51:34,132] Trial 0 finished with value: 0.5784313725490196 and parameters: {'lr': 0.00017893271132792836, 'model_type': 'Transformer', 'num_heads': 4, 'hidden_dim': 126, 'num_layers': 3}. Best is trial 0 with value: 0.5784313725490196.\n",
      "[I 2024-12-05 21:51:43,271] Trial 1 finished with value: 0.5508578431372548 and parameters: {'lr': 0.0002211757147703584, 'model_type': 'Transformer', 'num_heads': 8, 'hidden_dim': 178, 'num_layers': 4}. Best is trial 0 with value: 0.5784313725490196.\n",
      "[I 2024-12-05 21:51:47,315] Trial 2 finished with value: 0.5784313725490196 and parameters: {'lr': 6.475451457387286e-05, 'model_type': 'Transformer', 'num_heads': 8, 'hidden_dim': 194, 'num_layers': 2}. Best is trial 0 with value: 0.5784313725490196.\n",
      "[I 2024-12-05 21:51:50,843] Trial 3 finished with value: 0.5430964052287582 and parameters: {'lr': 6.603878395923621e-05, 'model_type': 'LSTM', 'hidden_dim': 193}. Best is trial 0 with value: 0.5784313725490196.\n",
      "[I 2024-12-05 21:51:57,469] Trial 4 finished with value: 0.5784313725490196 and parameters: {'lr': 8.338622489174235e-05, 'model_type': 'Transformer', 'num_heads': 8, 'hidden_dim': 97, 'num_layers': 3}. Best is trial 0 with value: 0.5784313725490196.\n",
      "[I 2024-12-05 21:52:01,308] Trial 5 finished with value: 0.5612745098039216 and parameters: {'lr': 9.723691386553037e-05, 'model_type': 'LSTM', 'hidden_dim': 160}. Best is trial 0 with value: 0.5784313725490196.\n",
      "[I 2024-12-05 21:52:03,749] Trial 6 finished with value: 0.5833333333333333 and parameters: {'lr': 5.7748868701010604e-05, 'model_type': 'LSTM', 'hidden_dim': 90}. Best is trial 6 with value: 0.5833333333333333.\n",
      "[I 2024-12-05 21:52:06,747] Trial 7 finished with value: 0.5416666666666666 and parameters: {'lr': 7.735243571922099e-05, 'model_type': 'LSTM', 'hidden_dim': 76}. Best is trial 6 with value: 0.5833333333333333.\n",
      "[I 2024-12-05 21:52:10,453] Trial 8 finished with value: 0.46568627450980393 and parameters: {'lr': 0.00018329697487462943, 'model_type': 'LSTM', 'hidden_dim': 150}. Best is trial 6 with value: 0.5833333333333333.\n",
      "[I 2024-12-05 21:52:13,587] Trial 9 finished with value: 0.5784313725490196 and parameters: {'lr': 0.0004090578918340141, 'model_type': 'LSTM', 'hidden_dim': 124}. Best is trial 6 with value: 0.5833333333333333.\n",
      "[I 2024-12-05 21:52:16,858] Trial 10 finished with value: 0.5784313725490196 and parameters: {'lr': 1.3370996512997748e-05, 'model_type': 'LSTM', 'hidden_dim': 69}. Best is trial 6 with value: 0.5833333333333333.\n",
      "[I 2024-12-05 21:52:21,273] Trial 11 finished with value: 0.5202205882352942 and parameters: {'lr': 2.665715104695904e-05, 'model_type': 'Transformer', 'num_heads': 4, 'hidden_dim': 232, 'num_layers': 1}. Best is trial 6 with value: 0.5833333333333333.\n",
      "[I 2024-12-05 21:52:32,529] Trial 12 finished with value: 0.5508578431372548 and parameters: {'lr': 0.0007485218082376007, 'model_type': 'Transformer', 'num_heads': 4, 'hidden_dim': 118, 'num_layers': 4}. Best is trial 6 with value: 0.5833333333333333.\n",
      "[I 2024-12-05 21:52:38,258] Trial 13 finished with value: 0.5857843137254902 and parameters: {'lr': 2.494400988784034e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 118, 'num_layers': 2}. Best is trial 13 with value: 0.5857843137254902.\n",
      "[I 2024-12-05 21:52:42,185] Trial 14 finished with value: 0.5508578431372548 and parameters: {'lr': 2.5579085011315543e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 98, 'num_layers': 1}. Best is trial 13 with value: 0.5857843137254902.\n",
      "[I 2024-12-05 21:52:45,708] Trial 15 finished with value: 0.5508578431372548 and parameters: {'lr': 3.439307622487051e-05, 'model_type': 'LSTM', 'hidden_dim': 95}. Best is trial 13 with value: 0.5857843137254902.\n",
      "[I 2024-12-05 21:52:49,930] Trial 16 finished with value: 0.5784313725490196 and parameters: {'lr': 1.1525976309813482e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 143, 'num_layers': 2}. Best is trial 13 with value: 0.5857843137254902.\n",
      "[I 2024-12-05 21:52:54,059] Trial 17 finished with value: 0.5606617647058822 and parameters: {'lr': 4.450082470322032e-05, 'model_type': 'LSTM', 'hidden_dim': 243}. Best is trial 13 with value: 0.5857843137254902.\n",
      "[I 2024-12-05 21:52:58,799] Trial 18 finished with value: 0.6060049019607844 and parameters: {'lr': 1.7738788851617893e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 84, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:53:03,056] Trial 19 finished with value: 0.6060049019607844 and parameters: {'lr': 1.6793554215195607e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 113, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:53:08,455] Trial 20 finished with value: 0.5784313725490196 and parameters: {'lr': 1.6871066054827472e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 78, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:53:13,883] Trial 21 finished with value: 0.5784313725490196 and parameters: {'lr': 1.9177053080298562e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 117, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:53:20,278] Trial 22 finished with value: 0.5508578431372548 and parameters: {'lr': 1.006459678301832e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 64, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:53:27,357] Trial 23 finished with value: 0.5784313725490196 and parameters: {'lr': 2.058579108341948e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 111, 'num_layers': 3}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:53:31,654] Trial 24 finished with value: 0.4632352941176471 and parameters: {'lr': 3.408468332712074e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 138, 'num_layers': 1}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:53:37,449] Trial 25 finished with value: 0.5508578431372548 and parameters: {'lr': 1.5658001923520247e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 104, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:53:43,984] Trial 26 finished with value: 0.5508578431372548 and parameters: {'lr': 4.2370592145632525e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 82, 'num_layers': 3}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:53:48,014] Trial 27 finished with value: 0.5563725490196078 and parameters: {'lr': 2.879390243052268e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 135, 'num_layers': 1}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:53:52,615] Trial 28 finished with value: 0.5784313725490196 and parameters: {'lr': 2.2566936200187757e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 164, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:53:57,371] Trial 29 finished with value: 0.6060049019607844 and parameters: {'lr': 0.00013081893815548306, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 131, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:54:06,436] Trial 30 finished with value: 0.5508578431372548 and parameters: {'lr': 0.0001534797865424647, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 127, 'num_layers': 3}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:54:12,714] Trial 31 finished with value: 0.5759803921568627 and parameters: {'lr': 0.00013804917287791967, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 108, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:54:17,621] Trial 32 finished with value: 0.5784313725490196 and parameters: {'lr': 0.000355881528739488, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 129, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:54:21,938] Trial 33 finished with value: 0.5784313725490196 and parameters: {'lr': 1.408479212520711e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 89, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:54:26,282] Trial 34 finished with value: 0.5784313725490196 and parameters: {'lr': 0.000270514758244419, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 153, 'num_layers': 1}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:54:30,545] Trial 35 finished with value: 0.5784313725490196 and parameters: {'lr': 5.035652854804937e-05, 'model_type': 'Transformer', 'num_heads': 4, 'hidden_dim': 173, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:54:35,672] Trial 36 finished with value: 0.571078431372549 and parameters: {'lr': 0.00010677407610490373, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 106, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:54:42,761] Trial 37 finished with value: 0.6060049019607844 and parameters: {'lr': 0.00011601743443837993, 'model_type': 'Transformer', 'num_heads': 8, 'hidden_dim': 197, 'num_layers': 3}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:54:50,610] Trial 38 finished with value: 0.5508578431372548 and parameters: {'lr': 0.00012304540418350452, 'model_type': 'Transformer', 'num_heads': 8, 'hidden_dim': 211, 'num_layers': 3}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:54:57,927] Trial 39 finished with value: 0.5784313725490196 and parameters: {'lr': 0.00021618187529928148, 'model_type': 'Transformer', 'num_heads': 8, 'hidden_dim': 220, 'num_layers': 3}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:55:04,164] Trial 40 finished with value: 0.5784313725490196 and parameters: {'lr': 8.128082879607297e-05, 'model_type': 'Transformer', 'num_heads': 8, 'hidden_dim': 200, 'num_layers': 3}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:55:10,379] Trial 41 finished with value: 0.5281862745098039 and parameters: {'lr': 6.568215732490363e-05, 'model_type': 'Transformer', 'num_heads': 8, 'hidden_dim': 184, 'num_layers': 2}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:55:18,448] Trial 42 finished with value: 0.6060049019607844 and parameters: {'lr': 0.00010023452785966394, 'model_type': 'Transformer', 'num_heads': 4, 'hidden_dim': 145, 'num_layers': 4}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:55:25,548] Trial 43 finished with value: 0.5508578431372548 and parameters: {'lr': 9.112357992480099e-05, 'model_type': 'Transformer', 'num_heads': 4, 'hidden_dim': 149, 'num_layers': 4}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:55:34,671] Trial 44 finished with value: 0.5508578431372548 and parameters: {'lr': 0.00017423370896628714, 'model_type': 'Transformer', 'num_heads': 4, 'hidden_dim': 166, 'num_layers': 4}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:55:44,235] Trial 45 finished with value: 0.5508578431372548 and parameters: {'lr': 0.00011131489044571811, 'model_type': 'Transformer', 'num_heads': 4, 'hidden_dim': 158, 'num_layers': 4}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:55:47,170] Trial 46 finished with value: 0.4859068627450981 and parameters: {'lr': 6.703071385046392e-05, 'model_type': 'LSTM', 'hidden_dim': 140}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:55:54,399] Trial 47 finished with value: 0.42156862745098034 and parameters: {'lr': 0.0002536802650770195, 'model_type': 'Transformer', 'num_heads': 8, 'hidden_dim': 181, 'num_layers': 4}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:56:01,628] Trial 48 finished with value: 0.5784313725490196 and parameters: {'lr': 0.0004891066941842922, 'model_type': 'Transformer', 'num_heads': 4, 'hidden_dim': 192, 'num_layers': 3}. Best is trial 18 with value: 0.6060049019607844.\n",
      "[I 2024-12-05 21:56:04,557] Trial 49 finished with value: 0.5285947712418301 and parameters: {'lr': 0.00014949426279677478, 'model_type': 'LSTM', 'hidden_dim': 132}. Best is trial 18 with value: 0.6060049019607844.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 1.7738788851617893e-05, 'model_type': 'Transformer', 'num_heads': 2, 'hidden_dim': 84, 'num_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial, train_game_loaders, val_game_loaders, device):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter tuning using Optuna.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): A single trial for Optuna optimization.\n",
    "        train_game_loaders (dict): Game-based training DataLoaders.\n",
    "        val_game_loaders (dict): Game-based validation DataLoaders.\n",
    "        device: Computation device (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        float: Validation accuracy.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    input_dim = 768\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    model_type = trial.suggest_categorical(\"model_type\", [\"LSTM\", \"Transformer\"])\n",
    "\n",
    "    if model_type == \"LSTM\":\n",
    "        hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "        model = ChronologicalLSTM(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "    else:\n",
    "        possible_heads = [h for h in [2, 4, 8] if input_dim % h == 0]  # Restrict to valid num_heads\n",
    "        num_heads = trial.suggest_categorical(\"num_heads\", possible_heads)\n",
    "        hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "        if hidden_dim % num_heads != 0:\n",
    "            hidden_dim -= hidden_dim % num_heads  # Adjust hidden_dim to be divisible by num_heads\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "        model = TransformerEncoder(input_dim=input_dim, hidden_dim=hidden_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(10):  # For faster tuning, use fewer epochs\n",
    "        train_loss = 0\n",
    "        for match_id, game_loader in train_game_loaders.items():\n",
    "            for embeddings, labels in game_loader:\n",
    "                embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(embeddings).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    total_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for match_id, val_loader in val_game_loaders.items():\n",
    "            for embeddings, labels in val_loader:\n",
    "                embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "                outputs = model(embeddings).squeeze()\n",
    "                val_accuracy += accuracy_score(labels.cpu().numpy(), (outputs.cpu().numpy() > 0.5))\n",
    "                total_batches += 1\n",
    "\n",
    "    return val_accuracy / total_batches\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, train_game_loaders, val_game_loaders, device), n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Transformer model with hidden_dim=84, num_heads=2, num_layers=2\n",
      "Epoch 1/60...\n",
      "Epoch 1: Train Loss 0.7228, Train Acc 0.4464\n",
      "Validation Loss: 0.7270, Validation Acc 0.4216\n",
      "Epoch 2/60...\n",
      "Epoch 2: Train Loss 0.7045, Train Acc 0.4720\n",
      "Validation Loss: 0.7083, Validation Acc 0.4216\n",
      "Epoch 3/60...\n",
      "Epoch 3: Train Loss 0.6958, Train Acc 0.4796\n",
      "Validation Loss: 0.6939, Validation Acc 0.4957\n",
      "Epoch 4/60...\n",
      "Epoch 4: Train Loss 0.6924, Train Acc 0.5248\n",
      "Validation Loss: 0.6900, Validation Acc 0.5509\n",
      "Epoch 5/60...\n",
      "Epoch 5: Train Loss 0.6913, Train Acc 0.5329\n",
      "Validation Loss: 0.6887, Validation Acc 0.5509\n",
      "Epoch 6/60...\n",
      "Epoch 6: Train Loss 0.6927, Train Acc 0.5239\n",
      "Validation Loss: 0.6892, Validation Acc 0.5509\n",
      "Epoch 7/60...\n",
      "Epoch 7: Train Loss 0.6910, Train Acc 0.5330\n",
      "Validation Loss: 0.6856, Validation Acc 0.5784\n",
      "Epoch 8/60...\n",
      "Epoch 8: Train Loss 0.6909, Train Acc 0.5355\n",
      "Validation Loss: 0.6880, Validation Acc 0.5509\n",
      "Epoch 9/60...\n",
      "Epoch 9: Train Loss 0.6920, Train Acc 0.5178\n",
      "Validation Loss: 0.6895, Validation Acc 0.5509\n",
      "Epoch 10/60...\n",
      "Epoch 10: Train Loss 0.6918, Train Acc 0.5265\n",
      "Validation Loss: 0.6848, Validation Acc 0.5784\n",
      "Epoch 11/60...\n",
      "Epoch 11: Train Loss 0.6920, Train Acc 0.5266\n",
      "Validation Loss: 0.6839, Validation Acc 0.5784\n",
      "Epoch 12/60...\n",
      "Epoch 12: Train Loss 0.6913, Train Acc 0.5286\n",
      "Validation Loss: 0.6876, Validation Acc 0.5509\n",
      "Epoch 13/60...\n",
      "Epoch 13: Train Loss 0.6899, Train Acc 0.5332\n",
      "Validation Loss: 0.6842, Validation Acc 0.5784\n",
      "Epoch 14/60...\n",
      "Epoch 14: Train Loss 0.6896, Train Acc 0.5395\n",
      "Validation Loss: 0.6867, Validation Acc 0.5509\n",
      "Epoch 15/60...\n",
      "Epoch 15: Train Loss 0.6902, Train Acc 0.5355\n",
      "Validation Loss: 0.6842, Validation Acc 0.5784\n",
      "Epoch 16/60...\n",
      "Epoch 16: Train Loss 0.6853, Train Acc 0.5515\n",
      "Validation Loss: 0.6856, Validation Acc 0.5509\n",
      "Epoch 17/60...\n",
      "Epoch 17: Train Loss 0.6896, Train Acc 0.5318\n",
      "Validation Loss: 0.6844, Validation Acc 0.5784\n",
      "Epoch 18/60...\n",
      "Epoch 18: Train Loss 0.6893, Train Acc 0.5281\n",
      "Validation Loss: 0.6854, Validation Acc 0.5784\n",
      "Epoch 19/60...\n",
      "Epoch 19: Train Loss 0.6893, Train Acc 0.5300\n",
      "Validation Loss: 0.6877, Validation Acc 0.5355\n",
      "Epoch 20/60...\n",
      "Epoch 20: Train Loss 0.6855, Train Acc 0.5438\n",
      "Validation Loss: 0.6775, Validation Acc 0.6060\n",
      "Epoch 21/60...\n",
      "Epoch 21: Train Loss 0.6897, Train Acc 0.5342\n",
      "Validation Loss: 0.6863, Validation Acc 0.5674\n",
      "Epoch 22/60...\n",
      "Epoch 22: Train Loss 0.6834, Train Acc 0.5540\n",
      "Validation Loss: 0.6801, Validation Acc 0.5784\n",
      "Epoch 23/60...\n",
      "Epoch 23: Train Loss 0.6892, Train Acc 0.5299\n",
      "Validation Loss: 0.6842, Validation Acc 0.5484\n",
      "Epoch 24/60...\n",
      "Epoch 24: Train Loss 0.6814, Train Acc 0.5591\n",
      "Validation Loss: 0.6831, Validation Acc 0.5509\n",
      "Epoch 25/60...\n",
      "Epoch 25: Train Loss 0.6865, Train Acc 0.5291\n",
      "Validation Loss: 0.6833, Validation Acc 0.5386\n",
      "Epoch 26/60...\n",
      "Epoch 26: Train Loss 0.6840, Train Acc 0.5320\n",
      "Validation Loss: 0.6835, Validation Acc 0.5637\n",
      "Epoch 27/60...\n",
      "Epoch 27: Train Loss 0.6809, Train Acc 0.5568\n",
      "Validation Loss: 0.6804, Validation Acc 0.5674\n",
      "Epoch 28/60...\n",
      "Epoch 28: Train Loss 0.6873, Train Acc 0.5406\n",
      "Validation Loss: 0.6868, Validation Acc 0.5637\n",
      "Epoch 29/60...\n",
      "Epoch 29: Train Loss 0.6810, Train Acc 0.5733\n",
      "Validation Loss: 0.6789, Validation Acc 0.5594\n",
      "Epoch 30/60...\n",
      "Epoch 30: Train Loss 0.6832, Train Acc 0.5497\n",
      "Validation Loss: 0.6806, Validation Acc 0.6091\n",
      "Epoch 31/60...\n",
      "Epoch 31: Train Loss 0.6789, Train Acc 0.5760\n",
      "Validation Loss: 0.6790, Validation Acc 0.5600\n",
      "Epoch 32/60...\n",
      "Epoch 32: Train Loss 0.6810, Train Acc 0.5602\n",
      "Validation Loss: 0.6786, Validation Acc 0.5699\n",
      "Epoch 33/60...\n",
      "Epoch 33: Train Loss 0.6774, Train Acc 0.5628\n",
      "Validation Loss: 0.6803, Validation Acc 0.5987\n",
      "Epoch 34/60...\n",
      "Epoch 34: Train Loss 0.6759, Train Acc 0.5875\n",
      "Validation Loss: 0.6825, Validation Acc 0.5790\n",
      "Epoch 35/60...\n",
      "Epoch 35: Train Loss 0.6786, Train Acc 0.5665\n",
      "Validation Loss: 0.6784, Validation Acc 0.6011\n",
      "Epoch 36/60...\n",
      "Epoch 36: Train Loss 0.6782, Train Acc 0.5771\n",
      "Validation Loss: 0.6859, Validation Acc 0.5558\n",
      "Epoch 37/60...\n",
      "Epoch 37: Train Loss 0.6763, Train Acc 0.5729\n",
      "Validation Loss: 0.6767, Validation Acc 0.6060\n",
      "Epoch 38/60...\n",
      "Epoch 38: Train Loss 0.6776, Train Acc 0.5766\n",
      "Validation Loss: 0.6778, Validation Acc 0.6085\n",
      "Epoch 39/60...\n",
      "Epoch 39: Train Loss 0.6763, Train Acc 0.5722\n",
      "Validation Loss: 0.6772, Validation Acc 0.6183\n",
      "Epoch 40/60...\n",
      "Epoch 40: Train Loss 0.6733, Train Acc 0.5850\n",
      "Validation Loss: 0.6758, Validation Acc 0.5809\n",
      "Epoch 41/60...\n",
      "Epoch 41: Train Loss 0.6702, Train Acc 0.5767\n",
      "Validation Loss: 0.6786, Validation Acc 0.5735\n",
      "Epoch 42/60...\n",
      "Epoch 42: Train Loss 0.6762, Train Acc 0.5703\n",
      "Validation Loss: 0.6761, Validation Acc 0.6207\n",
      "Epoch 43/60...\n",
      "Epoch 43: Train Loss 0.6701, Train Acc 0.5950\n",
      "Validation Loss: 0.6793, Validation Acc 0.5882\n",
      "Epoch 44/60...\n",
      "Epoch 44: Train Loss 0.6753, Train Acc 0.5700\n",
      "Validation Loss: 0.6761, Validation Acc 0.5735\n",
      "Epoch 45/60...\n",
      "Epoch 45: Train Loss 0.6724, Train Acc 0.5958\n",
      "Validation Loss: 0.6784, Validation Acc 0.5460\n",
      "Epoch 46/60...\n",
      "Epoch 46: Train Loss 0.6744, Train Acc 0.5788\n",
      "Validation Loss: 0.6741, Validation Acc 0.6158\n",
      "Epoch 47/60...\n",
      "Epoch 47: Train Loss 0.6723, Train Acc 0.5809\n",
      "Validation Loss: 0.6727, Validation Acc 0.6036\n",
      "Epoch 48/60...\n",
      "Epoch 48: Train Loss 0.6688, Train Acc 0.5882\n",
      "Validation Loss: 0.6725, Validation Acc 0.5987\n",
      "Epoch 49/60...\n",
      "Epoch 49: Train Loss 0.6693, Train Acc 0.5984\n",
      "Validation Loss: 0.6734, Validation Acc 0.6134\n",
      "Epoch 50/60...\n",
      "Epoch 50: Train Loss 0.6698, Train Acc 0.5832\n",
      "Validation Loss: 0.6745, Validation Acc 0.6158\n",
      "Epoch 51/60...\n",
      "Epoch 51: Train Loss 0.6695, Train Acc 0.5947\n",
      "Validation Loss: 0.6783, Validation Acc 0.5907\n",
      "Epoch 52/60...\n",
      "Epoch 52: Train Loss 0.6653, Train Acc 0.5974\n",
      "Validation Loss: 0.6823, Validation Acc 0.5582\n",
      "Epoch 53/60...\n",
      "Epoch 53: Train Loss 0.6692, Train Acc 0.5765\n",
      "Validation Loss: 0.6770, Validation Acc 0.5858\n",
      "Epoch 54/60...\n",
      "Epoch 54: Train Loss 0.6709, Train Acc 0.5849\n",
      "Validation Loss: 0.6752, Validation Acc 0.5784\n",
      "Epoch 55/60...\n",
      "Epoch 55: Train Loss 0.6686, Train Acc 0.5928\n",
      "Validation Loss: 0.6751, Validation Acc 0.5760\n",
      "Epoch 56/60...\n",
      "Epoch 56: Train Loss 0.6761, Train Acc 0.5826\n",
      "Validation Loss: 0.6728, Validation Acc 0.6183\n",
      "Epoch 57/60...\n",
      "Epoch 57: Train Loss 0.6640, Train Acc 0.6071\n",
      "Validation Loss: 0.6715, Validation Acc 0.6036\n",
      "Epoch 58/60...\n",
      "Epoch 58: Train Loss 0.6675, Train Acc 0.5934\n",
      "Validation Loss: 0.6715, Validation Acc 0.6036\n",
      "Epoch 59/60...\n",
      "Epoch 59: Train Loss 0.6699, Train Acc 0.5877\n",
      "Validation Loss: 0.6752, Validation Acc 0.5760\n",
      "Epoch 60/60...\n",
      "Epoch 60: Train Loss 0.6668, Train Acc 0.6056\n",
      "Validation Loss: 0.6796, Validation Acc 0.5460\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "input_dim = 768\n",
    "hidden_dim = best_params[\"hidden_dim\"]\n",
    "lr = best_params[\"lr\"]\n",
    "\n",
    "if best_params[\"model_type\"] == \"LSTM\":\n",
    "    best_model = ChronologicalLSTM(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "    print(f\"Best LSTM model with hidden_dim={hidden_dim}\")\n",
    "else:\n",
    "    print(f\"Best Transformer model with hidden_dim={hidden_dim}, num_heads={best_params['num_heads']}, num_layers={best_params['num_layers']}\")\n",
    "    best_model = TransformerEncoder(\n",
    "        input_dim=input_dim, \n",
    "        hidden_dim=hidden_dim, \n",
    "        num_heads=best_params.get(\"num_heads\", 4), \n",
    "        num_layers=best_params.get(\"num_layers\", 2)\n",
    "    )\n",
    "\n",
    "# Train the best model\n",
    "trained_best_model = train_game_based_model(best_model, train_game_loaders, val_game_loaders, epochs=60, lr=lr, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Chronological LSTM...\n",
      "Evaluating Transformer Encoder...\n",
      "Evaluating Fine-Tuned Model...\n",
      "                 Model  Accuracy  Precision    Recall  F1-Score\n",
      "0   Chronological LSTM  0.719697   0.789216  0.703057  0.743649\n",
      "1  Transformer Encoder  0.714646   0.798969  0.676856  0.732861\n",
      "2     Fine-Tuned Model  0.578283   0.623016  0.685590  0.652807\n"
     ]
    }
   ],
   "source": [
    "# Include fine-tuned model in comparison\n",
    "comparison_df = evaluate_model_comparison(\n",
    "    {\n",
    "        \"Chronological LSTM\": trained_lstm_model,\n",
    "        \"Transformer Encoder\": trained_transformer_model,\n",
    "        \"Fine-Tuned Model\": trained_best_model\n",
    "    },\n",
    "    val_game_loaders, device\n",
    ")\n",
    "\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformerEncoder' object has no attribute 'hidden_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(predictions)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Generate predictions for submission\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# submission_df = predict_for_kaggle_submission(trained_lstm_model, test_df, device)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m submission_df \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_for_kaggle_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m submission_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmission file saved as submission.csv.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m, in \u001b[0;36mpredict_for_kaggle_submission\u001b[0;34m(model, test_df, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m embeddings_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(embeddings, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Check for mismatched input size\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m embeddings_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_dim\u001b[49m, (\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput size mismatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs model hidden_dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(embeddings_tensor)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     19\u001b[0m preds \u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/inf554/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransformerEncoder' object has no attribute 'hidden_dim'"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "def predict_for_kaggle_submission(model, test_df, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        grouped = test_df.groupby(\"MatchID\")\n",
    "        for match_id, group in grouped:\n",
    "            embeddings = np.vstack(group[\"aggregated_embedding\"].values)\n",
    "            embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Check for mismatched input size\n",
    "            assert embeddings_tensor.shape[1] == model.hidden_dim, (\n",
    "                f\"Input size mismatch: {embeddings_tensor.shape[1]} vs model hidden_dim {model.hidden_dim}\"\n",
    "            )\n",
    "            \n",
    "            outputs = model(embeddings_tensor).squeeze(-1)  # Add `-1` to handle trailing dimensions\n",
    "            preds = (outputs.cpu().numpy() > 0.5).astype(int)\n",
    "            group[\"EventType\"] = preds\n",
    "            predictions.append(group[[\"ID\", \"EventType\"]])\n",
    "\n",
    "    return pd.concat(predictions)\n",
    "\n",
    "\n",
    "# Generate predictions for submission\n",
    "# submission_df = predict_for_kaggle_submission(trained_lstm_model, test_df, device)\n",
    "submission_df = predict_for_kaggle_submission(trained_best_model, test_df, device)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file saved as submission.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChronologicalLSTM' object has no attribute 'hidden_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(predictions)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Generate predictions for submission\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m submission_df \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_for_kaggle_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_lstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m submission_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmission file saved as submission2.csv.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[61], line 14\u001b[0m, in \u001b[0;36mpredict_for_kaggle_submission\u001b[0;34m(model, test_df, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m embeddings_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(embeddings, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Check for mismatched input size\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m embeddings_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_dim\u001b[49m, (\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput size mismatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs model hidden_dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(embeddings_tensor)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add `-1` to handle trailing dimensions\u001b[39;00m\n\u001b[1;32m     19\u001b[0m preds \u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/inf554/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ChronologicalLSTM' object has no attribute 'hidden_dim'"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "def predict_for_kaggle_submission(model, test_df, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        grouped = test_df.groupby(\"MatchID\")\n",
    "        for match_id, group in grouped:\n",
    "            embeddings = np.vstack(group[\"aggregated_embedding\"].values)\n",
    "            embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Check for mismatched input size\n",
    "            assert embeddings_tensor.shape[1] == model.hidden_dim, (\n",
    "                f\"Input size mismatch: {embeddings_tensor.shape[1]} vs model hidden_dim {model.hidden_dim}\"\n",
    "            )\n",
    "            \n",
    "            outputs = model(embeddings_tensor).squeeze(-1)  # Add `-1` to handle trailing dimensions\n",
    "            preds = (outputs.cpu().numpy() > 0.5).astype(int)\n",
    "            group[\"EventType\"] = preds\n",
    "            predictions.append(group[[\"ID\", \"EventType\"]])\n",
    "\n",
    "    return pd.concat(predictions)\n",
    "\n",
    "# Generate predictions for submission\n",
    "submission_df = predict_for_kaggle_submission(trained_lstm_model, test_df, device)\n",
    "submission_df.to_csv(\"submission2.csv\", index=False)\n",
    "print(\"Submission file saved as submission2.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "def predict_for_kaggle_submission(model, test_df, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        grouped = test_df.groupby(\"MatchID\")\n",
    "        for match_id, group in grouped:\n",
    "            embeddings = np.vstack(group[\"aggregated_embedding\"].values)\n",
    "            embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Check for mismatched input size\n",
    "            assert embeddings_tensor.shape[1] == model.hidden_dim, (\n",
    "                f\"Input size mismatch: {embeddings_tensor.shape[1]} vs model hidden_dim {model.hidden_dim}\"\n",
    "            )\n",
    "            \n",
    "            outputs = model(embeddings_tensor).squeeze(-1)  # Add `-1` to handle trailing dimensions\n",
    "            preds = (outputs.cpu().numpy() > 0.5).astype(int)\n",
    "            group[\"EventType\"] = preds\n",
    "            predictions.append(group[[\"ID\", \"EventType\"]])\n",
    "\n",
    "    return pd.concat(predictions)\n",
    "\n",
    "# Generate predictions for submission\n",
    "submission_df = predict_for_kaggle_submission(trained_transformer_model, test_df, device)\n",
    "submission_df.to_csv(\"submission3.csv\", index=False)\n",
    "print(\"Submission file saved as submission3.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
